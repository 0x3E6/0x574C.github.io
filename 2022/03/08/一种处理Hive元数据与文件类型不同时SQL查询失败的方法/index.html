<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>一种处理Hive元数据与文件类型不同时SQL查询失败的方法 | 0x3E6的博客</title><meta name="description" content="通过修改Hive（2.1.1-cdh6.3.0）源码的方式，处理Hive元数据与文件类型不同时，SQL查询失败的问题，将类型不兼容的字段查询结果设置为空值。"><meta name="keywords" content="大数据,Hive"><meta name="author" content="0x3E6"><meta name="copyright" content="0x3E6"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin><link rel="preconnect" href="//busuanzi.ibruce.info"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="一种处理Hive元数据与文件类型不同时SQL查询失败的方法"><meta name="twitter:description" content="通过修改Hive（2.1.1-cdh6.3.0）源码的方式，处理Hive元数据与文件类型不同时，SQL查询失败的问题，将类型不兼容的字段查询结果设置为空值。"><meta name="twitter:image" content="http://longwang.liveimg/bg-images/Agatsuma Zenitsu.jpg"><meta property="og:type" content="article"><meta property="og:title" content="一种处理Hive元数据与文件类型不同时SQL查询失败的方法"><meta property="og:url" content="http://longwang.live/2022/03/08/%E4%B8%80%E7%A7%8D%E5%A4%84%E7%90%86Hive%E5%85%83%E6%95%B0%E6%8D%AE%E4%B8%8E%E6%96%87%E4%BB%B6%E7%B1%BB%E5%9E%8B%E4%B8%8D%E5%90%8C%E6%97%B6SQL%E6%9F%A5%E8%AF%A2%E5%A4%B1%E8%B4%A5%E7%9A%84%E6%96%B9%E6%B3%95/"><meta property="og:site_name" content="0x3E6的博客"><meta property="og:description" content="通过修改Hive（2.1.1-cdh6.3.0）源码的方式，处理Hive元数据与文件类型不同时，SQL查询失败的问题，将类型不兼容的字段查询结果设置为空值。"><meta property="og:image" content="http://longwang.liveimg/bg-images/Agatsuma Zenitsu.jpg"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>const autoChangeMode = 'false'
var t = Cookies.get("theme");
if (autoChangeMode == '1'){
const isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
const isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
const isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

if (t === undefined){
  if (isLightMode) activateLightMode()
  else if (isDarkMode) activateDarkMode()
  else if (isNotSpecified || hasNoSupport){
    console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
    now = new Date();
    hour = now.getHours();
    isNight = hour < 6 || hour >= 18
    isNight ? activateDarkMode() : activateLightMode()
}
} else if (t == 'light') activateLightMode()
else activateDarkMode()


} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="canonical" href="http://longwang.live/2022/03/08/%E4%B8%80%E7%A7%8D%E5%A4%84%E7%90%86Hive%E5%85%83%E6%95%B0%E6%8D%AE%E4%B8%8E%E6%96%87%E4%BB%B6%E7%B1%BB%E5%9E%8B%E4%B8%8D%E5%90%8C%E6%97%B6SQL%E6%9F%A5%E8%AF%A2%E5%A4%B1%E8%B4%A5%E7%9A%84%E6%96%B9%E6%B3%95/"><link rel="prev" title="一种处理Hive元数据与文件类型不同时SQL查询失败的方法（二）" href="http://longwang.live/2022/03/24/%E4%B8%80%E7%A7%8D%E5%A4%84%E7%90%86Hive%E5%85%83%E6%95%B0%E6%8D%AE%E4%B8%8E%E6%96%87%E4%BB%B6%E7%B1%BB%E5%9E%8B%E4%B8%8D%E5%90%8C%E6%97%B6SQL%E6%9F%A5%E8%AF%A2%E5%A4%B1%E8%B4%A5%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%88%E4%BA%8C%EF%BC%89/"><link rel="next" title="Hive源码调试" href="http://longwang.live/2021/03/23/Hive%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    title: 'Snackbar.bookmark.title',
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  baiduPush: false,
  isHome: false,
  isPost: true
  
}</script><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="0x3E6的博客" type="application/atom+xml">
</head><body><header> <div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">0x3E6的博客</a></span><span class="toggle-menu pull_right close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-list" aria-hidden="true"></i><span> List</span><i class="fa fa-chevron-down menus-expand" aria-hidden="true"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fa fa-music"></i><span> Music</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fa fa-film"></i><span> Movie</span></a></li></ul></div></div></span></div></header><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">6</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">5</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-list" aria-hidden="true"></i><span> List</span><i class="fa fa-chevron-down menus-expand" aria-hidden="true"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fa fa-music"></i><span> Music</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fa fa-film"></i><span> Movie</span></a></li></ul></div></div></div><div id="mobile-sidebar-toc"><div class="toc_mobile_headline">目录</div><div class="sidebar-toc__content"><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text">一、背景</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link"><span class="toc_mobile_items-number">2.</span> <span class="toc_mobile_items-text">二、分析过程</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link"><span class="toc_mobile_items-number">2.1.</span> <span class="toc_mobile_items-text">2.1 环境及测试数据</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link"><span class="toc_mobile_items-number">2.1.1.</span> <span class="toc_mobile_items-text">2.1.1 环境</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link"><span class="toc_mobile_items-number">2.1.2.</span> <span class="toc_mobile_items-text">2.1.2 测试数据</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link"><span class="toc_mobile_items-number">2.2.</span> <span class="toc_mobile_items-text">2.2 select语句异常分析</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link"><span class="toc_mobile_items-number">2.2.1.</span> <span class="toc_mobile_items-text">2.2.1 异常分析</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link"><span class="toc_mobile_items-number">2.2.2.</span> <span class="toc_mobile_items-text">2.2.2 捕获异常位置</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link"><span class="toc_mobile_items-number">2.3.</span> <span class="toc_mobile_items-text">2.3 insert overwrite语句异常分析</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link"><span class="toc_mobile_items-number">2.3.1.</span> <span class="toc_mobile_items-text">2.3.1 异常分析</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link"><span class="toc_mobile_items-number">2.3.2.</span> <span class="toc_mobile_items-text">2.3.2 捕获异常位置</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link"><span class="toc_mobile_items-number">2.3.3.</span> <span class="toc_mobile_items-text">2.3.2 读数据（readRow）异常</span></a></li></ol></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link"><span class="toc_mobile_items-number">3.</span> <span class="toc_mobile_items-text">三、代码示例及结论</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link"><span class="toc_mobile_items-number">4.</span> <span class="toc_mobile_items-text">四、CDH集群中部署修改后的jar包</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link"><span class="toc_mobile_items-number">5.</span> <span class="toc_mobile_items-text">五、可能的其他方案？</span></a></li></ol></div></div></div><div id="body-wrap"><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true">     </i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">一、背景</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">二、分析过程</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">2.1.</span> <span class="toc-text">2.1 环境及测试数据</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">2.1.1.</span> <span class="toc-text">2.1.1 环境</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">2.1.2.</span> <span class="toc-text">2.1.2 测试数据</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">2.2.</span> <span class="toc-text">2.2 select语句异常分析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">2.2.1.</span> <span class="toc-text">2.2.1 异常分析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">2.2.2.</span> <span class="toc-text">2.2.2 捕获异常位置</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">2.3.</span> <span class="toc-text">2.3 insert overwrite语句异常分析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">2.3.1.</span> <span class="toc-text">2.3.1 异常分析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">2.3.2.</span> <span class="toc-text">2.3.2 捕获异常位置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">2.3.3.</span> <span class="toc-text">2.3.2 读数据（readRow）异常</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">3.</span> <span class="toc-text">三、代码示例及结论</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">4.</span> <span class="toc-text">四、CDH集群中部署修改后的jar包</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">5.</span> <span class="toc-text">五、可能的其他方案？</span></a></li></ol></div></div></div><main id="content-outer"><div id="top-container" style="background-image: url(img/bg-images/Agatsuma Zenitsu.jpg)"><div id="post-info"><div id="post-title"><div class="posttitle">一种处理Hive元数据与文件类型不同时SQL查询失败的方法</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 发表于 2022-03-08<span class="post-meta__separator">|</span><i class="fa fa-history fa-fw" aria-hidden="true"></i> 更新于 2022-03-26</time><div class="post-meta-wordcount"><div class="post-meta-pv-cv"><span><i class="fa fa-eye post-meta__icon fa-fw" aria-hidden="true"> </i>阅读量:</span><span id="busuanzi_value_page_pv"></span></div></div></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><h1><span id="一-背景">一、背景</span></h1><p>&ensp;&ensp;&ensp;&ensp;由于Hive的元数据与文件存储分离，且可单独修改表的类型，造成元数据与文件类型不同，这时使用SQL查询数据则会报错。不幸的是，我们就有这种需求，数采的数据同步了一份在Hive中，每天有大量的数据实时写入生成大量小文件；且对列的类型修改等没做限制，列类型可以被改成与之前不兼容的类型，以致于查询报错，通过insert overwrite来合并小文件的任务也一直失败，HDFS上小文件不断增多，严重影响查询效率。<br>&ensp;&ensp;&ensp;&ensp;当前使用的Hive版本为2.1.1-cdh6.3.0（CDH6.3.0），搜索了一下也没有什么配置可以直接让Hive忽略这种类型不同的错误，当前Hive版本也没有高版本似乎有的类型兼容的功能；先简单调试了一下发现Hive Hook功能似乎也拦截不到数据这一步。不得已尝试一下修改源码的方式，却走通了。<br>&ensp;&ensp;&ensp;&ensp;本文记录通过修改Hive（2.1.1-cdh6.3.0）源码的方式，处理Hive元数据与文件类型不同时，SQL查询失败的问题，将类型不兼容的字段查询结果设置为空值。</p>
<h1><span id="二-分析过程">二、分析过程</span></h1><h2><span id="21-环境及测试数据">2.1 环境及测试数据</span></h2><h3><span id="211-环境">2.1.1 环境</span></h3><p>&ensp;&ensp;&ensp;&ensp;CDH6.3.0，Hive版本为2.1.1-cdh6.3.0，还是调试hiveserver2，调试方法参考之前的《Hive源码调试》文章。顺带一提，github上cloudera/hive已经搜不到了，可能不打算开源了，还好gitee上这位朋友保存了一份<a target="_blank" rel="noopener" href="https://gitee.com/gabry/cloudera-hive">https://gitee.com/gabry/cloudera-hive</a> ，有需要的可以自己保存一下这个仓库。</p>
<h3><span id="212-测试数据">2.1.2 测试数据</span></h3><p>创建一个表t1（我们默认用的parquet格式，本文也只测试过parquet格式数据；分区表也可以，但这里只举一个非分区表例子），插入两条数据；再创建一个列名相同，但id列类型不同的表error_type：</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">sql</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> t1(id <span class="type">float</span>,content string) stored <span class="keyword">as</span> parquet;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> t1 vlaues(<span class="number">1.1</span>,<span class="string">&#x27;content1&#x27;</span>),(<span class="number">2.2</span>,<span class="string">&#x27;content2&#x27;</span>);</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> error_type(id <span class="type">int</span>,content string) stored <span class="keyword">as</span> parquet;</span><br></pre></td></tr></table></figure></div>
<p>在HDFS上直接将t1的数据文件拷到error_type表的目录下：</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">shell</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -cp /user/hive/warehouse/testdb.db/t1/000000_0 /user/hive/warehouse/testdb.db/error_type/</span><br></pre></td></tr></table></figure></div>
<p>这时使用sql查询error_type表则会报错：</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2:&#x2F;&#x2F;localhost:10000&gt; select * from error_type;</span><br><span class="line">INFO  : Compiling command(queryId&#x3D;hive_20220306113526_62d5507c-8df1-478b-8f9f-4ea1b8601df9): select * from error_type</span><br><span class="line">INFO  : Semantic Analysis Completed</span><br><span class="line">INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:error_type.id, type:int, comment:null), FieldSchema(name:error_type.content, type:string, comment:null)], properties:null)</span><br><span class="line">INFO  : Completed compiling command(queryId&#x3D;hive_20220306113526_62d5507c-8df1-478b-8f9f-4ea1b8601df9); Time taken: 0.13 seconds</span><br><span class="line">INFO  : Executing command(queryId&#x3D;hive_20220306113526_62d5507c-8df1-478b-8f9f-4ea1b8601df9): select * from error_type</span><br><span class="line">INFO  : Completed executing command(queryId&#x3D;hive_20220306113526_62d5507c-8df1-478b-8f9f-4ea1b8601df9); Time taken: 0.001 seconds</span><br><span class="line">INFO  : OK</span><br><span class="line">Error: java.io.IOException: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassCastException: org.apache.hadoop.io.FloatWritable cannot be cast to org.apache.hadoop.io.IntWritable (state&#x3D;,code&#x3D;0)</span><br></pre></td></tr></table></figure></div>
<h2><span id="22-select语句异常分析">2.2 select语句异常分析</span></h2><h3><span id="221-异常分析">2.2.1 异常分析</span></h3><p>开始调试，上面的ClassCastException发生处的函数调用栈（从IDEA中复制）为：</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">getPrimitiveJavaObject:46, WritableIntObjectInspector (org.apache.hadoop.hive.serde2.objectinspector.primitive)</span><br><span class="line">copyToStandardObject:412, ObjectInspectorUtils (org.apache.hadoop.hive.serde2.objectinspector)</span><br><span class="line">toThriftPayload:170, SerDeUtils (org.apache.hadoop.hive.serde2)</span><br><span class="line">convert:49, ThriftFormatter (org.apache.hadoop.hive.serde2.thrift)</span><br><span class="line">process:94, ListSinkOperator (org.apache.hadoop.hive.ql.exec)</span><br><span class="line">forward:882, Operator (org.apache.hadoop.hive.ql.exec)</span><br><span class="line">process:95, SelectOperator (org.apache.hadoop.hive.ql.exec)</span><br><span class="line">forward:882, Operator (org.apache.hadoop.hive.ql.exec)</span><br><span class="line">process:130, TableScanOperator (org.apache.hadoop.hive.ql.exec)</span><br><span class="line">pushRow:438, FetchOperator (org.apache.hadoop.hive.ql.exec)</span><br><span class="line">pushRow:430, FetchOperator (org.apache.hadoop.hive.ql.exec)</span><br><span class="line">fetch:146, FetchTask (org.apache.hadoop.hive.ql.exec)</span><br><span class="line">getResults:2227, Driver (org.apache.hadoop.hive.ql)</span><br><span class="line">getNextRowSet:491, SQLOperation (org.apache.hive.service.cli.operation)</span><br><span class="line">getOperationNextRowSet:297, OperationManager (org.apache.hive.service.cli.operation)</span><br><span class="line">fetchResults:869, HiveSessionImpl (org.apache.hive.service.cli.session)</span><br><span class="line">invoke:-1, GeneratedMethodAccessor5 (sun.reflect)</span><br><span class="line">invoke:43, DelegatingMethodAccessorImpl (sun.reflect)</span><br><span class="line">invoke:498, Method (java.lang.reflect)</span><br><span class="line">invoke:78, HiveSessionProxy (org.apache.hive.service.cli.session)</span><br><span class="line">access$000:36, HiveSessionProxy (org.apache.hive.service.cli.session)</span><br><span class="line">run:63, HiveSessionProxy$1 (org.apache.hive.service.cli.session)</span><br><span class="line">doPrivileged:-1, AccessController (java.security)</span><br><span class="line">doAs:422, Subject (javax.security.auth)</span><br><span class="line">doAs:1962, UserGroupInformation (org.apache.hadoop.security)</span><br><span class="line">invoke:59, HiveSessionProxy (org.apache.hive.service.cli.session)</span><br><span class="line">fetchResults:-1, $Proxy39 (com.sun.proxy)</span><br><span class="line">fetchResults:507, CLIService (org.apache.hive.service.cli)</span><br><span class="line">FetchResults:708, ThriftCLIService (org.apache.hive.service.cli.thrift)</span><br><span class="line">getResult:1717, TCLIService$Processor$FetchResults (org.apache.hive.service.rpc.thrift)</span><br><span class="line">getResult:1702, TCLIService$Processor$FetchResults (org.apache.hive.service.rpc.thrift)</span><br><span class="line">process:39, ProcessFunction (org.apache.thrift)</span><br><span class="line">process:39, TBaseProcessor (org.apache.thrift)</span><br><span class="line">process:56, TSetIpAddressProcessor (org.apache.hive.service.auth)</span><br><span class="line">run:286, TThreadPoolServer$WorkerProcess (org.apache.thrift.server)</span><br><span class="line">runWorker:1149, ThreadPoolExecutor (java.util.concurrent)</span><br><span class="line">run:624, ThreadPoolExecutor$Worker (java.util.concurrent)</span><br><span class="line">run:748, Thread (java.lang)</span><br></pre></td></tr></table></figure></div>
<p>WritableIntObjectInspector.getPrimitiveJavaObject:46这个方法为：</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Object <span class="title">getPrimitiveJavaObject</span><span class="params">(Object o)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> o == <span class="keyword">null</span> ? <span class="keyword">null</span> : Integer.valueOf(((IntWritable) o).get());</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>
<p>此时参数为：</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">this &#x3D; &#123;WritableIntObjectInspector@10276&#125; </span><br><span class="line"> typeInfo &#x3D; &#123;PrimitiveTypeInfo@10277&#125; &quot;int&quot;</span><br><span class="line">o &#x3D; &#123;FloatWritable@10262&#125; &quot;1.1&quot;</span><br></pre></td></tr></table></figure></div>
<p>这里将从文件中读取的FloatWritable类型的对象，转换为根据表元数据int类型对应的IntWritable类型，出现ClassCastException。</p>
<p>对日志中看到的HiveException类的构造函数下断点，可知道抛出HiveException的位置为函数调用栈process:94, ListSinkOperator (org.apache.hadoop.hive.ql.exec)这一行对应的这个函数：</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line">  <span class="meta">@SuppressWarnings(&quot;unchecked&quot;)</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(Object row, <span class="keyword">int</span> tag)</span> <span class="keyword">throws</span> HiveException </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      res.add(fetcher.convert(row, inputObjInspectors[<span class="number">0</span>]));</span><br><span class="line">      numRows++;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> HiveException(e);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>
<h3><span id="222-捕获异常位置">2.2.2 捕获异常位置</span></h3><p>异常抛出后，被捕获并抛出HiveException之前的几个栈中函数：</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">getPrimitiveJavaObject:46, WritableIntObjectInspector (org.apache.hadoop.hive.serde2.objectinspector.primitive)</span><br><span class="line">copyToStandardObject:412, ObjectInspectorUtils (org.apache.hadoop.hive.serde2.objectinspector)</span><br><span class="line">toThriftPayload:170, SerDeUtils (org.apache.hadoop.hive.serde2)</span><br><span class="line">convert:49, ThriftFormatter (org.apache.hadoop.hive.serde2.thrift)</span><br></pre></td></tr></table></figure></div>
<p><code>getPrimitiveJavaObject:46, WritableIntObjectInspector</code>显然是特定类型的实现，不适合在这里捕获异常；<code>copyToStandardObject:412, ObjectInspectorUtils</code>函数本身逻辑比较复杂；<code>toThriftPayload:170, SerDeUtils</code>和<code>convert:49, ThriftFormatter</code>都可以，<code>convert:49, ThriftFormatter</code>刚好有个循环处理一行数据的每个字段，在这里处理看起来比较清晰，</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Object <span class="title">convert</span><span class="params">(Object row, ObjectInspector rowOI)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">  StructObjectInspector structOI = (StructObjectInspector) rowOI;</span><br><span class="line">  List&lt;? extends StructField&gt; fields = structOI.getAllStructFieldRefs();</span><br><span class="line">  Object[] converted = <span class="keyword">new</span> Object[fields.size()];</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span> ; i &lt; converted.length; i++) &#123;</span><br><span class="line">    StructField fieldRef = fields.get(i);</span><br><span class="line">    Object field = structOI.getStructFieldData(row, fieldRef);</span><br><span class="line">    converted[i] = field == <span class="keyword">null</span> ? <span class="keyword">null</span> :</span><br><span class="line">        SerDeUtils.toThriftPayload(field, fieldRef.getFieldObjectInspector(), protocol);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> converted;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
<p>将生成converted[i]的那行改为：</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">  converted[i] = field == <span class="keyword">null</span> ? <span class="keyword">null</span> :</span><br><span class="line">          SerDeUtils.toThriftPayload(field, fieldRef.getFieldObjectInspector(), protocol);</span><br><span class="line">&#125; <span class="keyword">catch</span> (ClassCastException e) &#123;</span><br><span class="line">    converted[i] = <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
<p>这样修改后（部署见后面章节）执行select * from error_type不会抛异常了，查询的2条数据id字段都为null。</p>
<h2><span id="23-insert-overwrite语句异常分析">2.3 insert overwrite语句异常分析</span></h2><h3><span id="231-异常分析">2.3.1 异常分析</span></h3><p>&ensp;&ensp;&ensp;&ensp;本以为就这样修改一下就可以了，尝试执行合并小文件的SQL：<code>insert overwrite table error_type select * from error_type</code>还会报错，日志里打印的函数调用栈如下：</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Caused by: java.lang.ClassCastException: org.apache.hadoop.io.FloatWritable cannot be cast to org.apache.hadoop.io.IntWritable</span><br><span class="line">	at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableIntObjectInspector.get(WritableIntObjectInspector.java:36)</span><br><span class="line">	at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter$IntDataWriter.write(DataWritableWriter.java:385)</span><br><span class="line">	at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter$GroupDataWriter.write(DataWritableWriter.java:199)</span><br><span class="line">	at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter$MessageDataWriter.write(DataWritableWriter.java:215)</span><br><span class="line">	at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter.write(DataWritableWriter.java:88)</span><br><span class="line">	at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriteSupport.write(DataWritableWriteSupport.java:60)</span><br><span class="line">	at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriteSupport.write(DataWritableWriteSupport.java:32)</span><br><span class="line">	at org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:123)</span><br><span class="line">	at org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:179)</span><br><span class="line">	at org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:46)</span><br><span class="line">	at org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper.write(ParquetRecordWriterWrapper.java:136)</span><br><span class="line">	at org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper.write(ParquetRecordWriterWrapper.java:149)</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.process(FileSinkOperator.java:769)</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:882)</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:95)</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:882)</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:130)</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:146)</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:484)</span><br></pre></td></tr></table></figure></div>
<h3><span id="232-捕获异常位置">2.3.2 捕获异常位置</span></h3><p>经过亿点调试分析（这些SQL有MR任务，任务会提交到Yarn，先设置参数<code>set hive.exec.mode.local.auto=true;</code>让Hive以本地模式运行该SQL，否则断点不会触发），接近抛异常位置（是否可以作为规律）的这个方法org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter.GroupDataWriter#write也有与前面<code>ThriftFormatter.convert:49</code>方法类似的通过循环写每一个字段的功能：</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(Object value)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; structFields.size(); i++) &#123;</span><br><span class="line">    StructField field = structFields.get(i);</span><br><span class="line">    Object fieldValue = inspector.getStructFieldData(value, field);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (fieldValue != <span class="keyword">null</span>) &#123;</span><br><span class="line">      String fieldName = field.getFieldName();</span><br><span class="line">      DataWriter writer = structWriters[i];</span><br><span class="line"></span><br><span class="line">      recordConsumer.startField(fieldName, i);</span><br><span class="line">      writer.write(fieldValue);</span><br><span class="line">      recordConsumer.endField(fieldName, i);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
<p>其中<code>writer.write(fieldValue)</code>就是异常信息打印的调用栈中的<code>org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter$GroupDataWriter.write(DataWritableWriter.java:199)</code>位置。</p>
<p>这里有一行<code>Object fieldValue = inspector.getStructFieldData(value, field);</code>，经过调试可以发现，这行代码和前面捕获异常的方法<code>convert:49, ThriftFormatter</code>中的<code>Object field = structOI.getStructFieldData(row, fieldRef);</code>调用的都是<code>org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector#getStructFieldData</code>：</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="meta">@SuppressWarnings(&quot;unchecked&quot;)</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Object <span class="title">getStructFieldData</span><span class="params">(Object data, StructField fieldRef)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (data == <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// We support both List&lt;Object&gt; and Object[]</span></span><br><span class="line">  <span class="comment">// so we have to do differently.</span></span><br><span class="line">  <span class="keyword">boolean</span> isArray = data.getClass().isArray();</span><br><span class="line">  <span class="keyword">if</span> (!isArray &amp;&amp; !(data <span class="keyword">instanceof</span> List)) &#123;</span><br><span class="line">    <span class="keyword">if</span> (!warned) &#123;</span><br><span class="line">      LOG.warn(<span class="string">&quot;Invalid type for struct &quot;</span> + data.getClass());</span><br><span class="line">      LOG.warn(<span class="string">&quot;ignoring similar errors.&quot;</span>);</span><br><span class="line">      warned = <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> data;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">int</span> listSize = (isArray ? ((Object[]) data).length : ((List&lt;Object&gt;) data)</span><br><span class="line">      .size());</span><br><span class="line">  MyField f = (MyField) fieldRef;</span><br><span class="line">  <span class="keyword">if</span> (fields.size() != listSize &amp;&amp; !warned) &#123;</span><br><span class="line">    <span class="comment">// <span class="doctag">TODO:</span> remove this</span></span><br><span class="line">    warned = <span class="keyword">true</span>;</span><br><span class="line">    LOG.warn(<span class="string">&quot;Trying to access &quot;</span> + fields.size()</span><br><span class="line">        + <span class="string">&quot; fields inside a list of &quot;</span> + listSize + <span class="string">&quot; elements: &quot;</span></span><br><span class="line">        + (isArray ? Arrays.asList((Object[]) data) : (List&lt;Object&gt;) data));</span><br><span class="line">    LOG.warn(<span class="string">&quot;ignoring similar errors.&quot;</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">int</span> fieldID = f.getFieldID();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (fieldID &gt;= listSize) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (isArray) &#123;</span><br><span class="line">    <span class="keyword">return</span> ((Object[]) data)[fieldID];</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> ((List&lt;Object&gt;) data).get(fieldID);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p><code>StandardStructObjectInspector#getStructFieldData</code>方法一个参数为从文件从读取的一行数据，第二个参数为<code>org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.MyField</code>的实例，其中有要取的字段的下标，这个方法大概功能就是根据下标从一行数据中取数，但是没做类型判断。<code>MyField</code>中也有与表的元数据中字段类型对应的ObjectInspector，可以使用ObjectInspector来读取一下本次获取的字段数据，如果类型冲突则捕获ClassCastException，并让本方法返回空值，后续的读写流程本字段都是null，这样无论对于之前的select语句还是insert overwrite语句，都可以达到本文想要的效果。</p>
<p>由于我们定义的Hive表都是用的原始类型，所以调用objectInspector实现的<code>org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector</code>接口中的方法<code>getPrimitiveJavaObject</code>，通过多态来实现各种类型数据的读取，<code>StandardStructObjectInspector#getStructFieldData</code>函数最后一个if else部分改为：</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">Object objValue;</span><br><span class="line"><span class="keyword">if</span> (fieldID &gt;= listSize) &#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> (isArray) &#123;</span><br><span class="line">  objValue = ((Object[]) data)[fieldID];</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  objValue = ((List&lt;Object&gt;) data).get(fieldID);</span><br><span class="line">&#125;</span><br><span class="line">ObjectInspector objectInspector = f.getFieldObjectInspector();</span><br><span class="line"><span class="keyword">if</span> (Category.PRIMITIVE.equals(objectInspector.getCategory())) &#123;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    ((PrimitiveObjectInspector) objectInspector).getPrimitiveJavaObject(objValue);</span><br><span class="line">  &#125; <span class="keyword">catch</span> (ClassCastException | UnsupportedOperationException e) &#123;</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    UnsupportedOperationException:</span></span><br><span class="line"><span class="comment">      如Hive列类型为String，这里获取到的objectInspector为ParquetStringInspector的实例，</span></span><br><span class="line"><span class="comment">      但org.apache.hadoop.hive.ql.io.parquet.serde.primitive.ParquetStringInspector.getPrimitiveJavaObject中，</span></span><br><span class="line"><span class="comment">      参数不是那个方法中做了判断的那几种类型时就会抛UnsupportedOperationException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    objValue = <span class="keyword">null</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> objValue;</span><br></pre></td></tr></table></figure></div>
<p>（修改了这部分后，对于原始类型，其实前面2.2.2节中的捕获异常可以删除）</p>
<h3><span id="232-读数据readrow异常">2.3.2 读数据（readRow）异常</span></h3><p>在有的表执行insert overwrite时，遇到了下面两个错误（其实是同一种）：</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Caused by: java.lang.UnsupportedOperationException: Cannot inspect org.apache.hadoop.io.LongWritable</span><br><span class="line">	at org.apache.hadoop.hive.ql.io.parquet.serde.primitive.ParquetStringInspector.getPrimitiveJavaObject(ParquetStringInspector.java:77)</span><br><span class="line">	at org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.getLong(PrimitiveObjectInspectorUtils.java:709)</span><br><span class="line">	at org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter$LongConverter.convert(PrimitiveObjectInspectorConverter.java:182)</span><br><span class="line">	at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters$StructConverter.convert(ObjectInspectorConverters.java:416)</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.readRow(MapOperator.java:126)</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.access$200(MapOperator.java:89)</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:483)</span><br></pre></td></tr></table></figure></div>

<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Caused by: java.lang.ClassCastException: org.apache.hadoop.io.FloatWritable cannot be cast to org.apache.hadoop.io.IntWritable</span><br><span class="line">	at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableIntObjectInspector.get(WritableIntObjectInspector.java:36)</span><br><span class="line">	at org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.getDouble(PrimitiveObjectInspectorUtils.java:755)</span><br><span class="line">	at org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.getFloat(PrimitiveObjectInspectorUtils.java:796)</span><br><span class="line">	at org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter$FloatConverter.convert(PrimitiveObjectInspectorConverter.java:211)</span><br><span class="line">	at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters$StructConverter.convert(ObjectInspectorConverters.java:416)</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.readRow(MapOperator.java:126)</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.access$200(MapOperator.java:89)</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:483)</span><br></pre></td></tr></table></figure></div>
<p>都是在MapOperator 483行调用MapOperator$MapOpCtx.readRow中的错误（2.3.1节中日志显示MapOperator 484调用MapOperator$MapOpCtx.forward中抛出的异常），只是两个日志中最后出错的数据类型不同。没分析代码，还是用与之前类似的方法，在<code>org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.StructConverter#convert</code>方法中也有个循环处理每个字段的功能，将异常日志中<code>ObjectInspectorConverters.java:416</code>指出的这一行代码：</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">outputFieldValue = fieldConverters.get(f).convert(inputFieldValue);</span><br></pre></td></tr></table></figure></div>
<p>改成：</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">  outputFieldValue = fieldConverters.get(f).convert(inputFieldValue);</span><br><span class="line">&#125; <span class="keyword">catch</span> (ClassCastException | UnsupportedOperationException e) &#123;</span><br><span class="line">  outputFieldValue = <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p><font color="red"><b>修改到这一步后，我们遇到的那些元数据与与文件不兼容类型的表都能正常查询了，也可以通过insert overwrite合并小文件了。</b></font></p>
<h1><span id="三-代码示例及结论">三、代码示例及结论</span></h1><p>修改的代码都提交到了fork的这个仓库中，可查看这个提交记录 <a target="_blank" rel="noopener" href="https://gitee.com/Ox3E6/cloudera-hive/commit/1e31127162b3bb29716580692c2d1fe30543f057">https://gitee.com/Ox3E6/cloudera-hive/commit/1e31127162b3bb29716580692c2d1fe30543f057</a></p>
<p>目前还不熟悉Hive代码迷宫中的细节和一些整体流程，仅仅根据报错位置尝试添加一些处理功能，如2.3.1节和2.3.3节都是在实际数据处理过程中抛出的异常。还是需要多做测试，遇到一个问题处理一个，只要转换成null后Hive的读写及其他逻辑不报错就行。</p>
<h1><span id="四-cdh集群中部署修改后的jar包">四、CDH集群中部署修改后的jar包</span></h1><p>&ensp;&ensp;&ensp;&ensp;修改的代码都位于hive-serde模块中，但是由于其他hive模块也引入了hive-serde依赖（没意识到这点之前，只替换了hive-serde jar包，每次还是抛出HiveException异常，重新编译调试了几次，甚至以为出现了灵异事件或是又要发现什么至今未知的Java异常捕获优先级技巧…），所以最简单的方法就是把编译打包后<font color="red"><b>lib目录下hive开头的jar包</b></font>全部拷过去。</p>
<p>CDH机器上，搜索hive jar包位置以hive-serde jar包为例：</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@dev-master2 ~]# find &#x2F; -name hive-serde-2.1.1-cdh6.3.0.jar</span><br><span class="line"></span><br><span class="line">&#x2F;opt&#x2F;cloudera&#x2F;cm&#x2F;cloudera-navigator-server&#x2F;libs&#x2F;cdh6&#x2F;hive-serde-2.1.1-cdh6.3.0.jar</span><br><span class="line">&#x2F;opt&#x2F;cloudera&#x2F;cm&#x2F;cloudera-scm-telepub&#x2F;libs&#x2F;cdh6&#x2F;hive-serde-2.1.1-cdh6.3.0.jar</span><br><span class="line">&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.3.0-1.cdh6.3.0.p0.1279813&#x2F;jars&#x2F;hive-serde-2.1.1-cdh6.3.0.jar</span><br><span class="line">&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.3.0-1.cdh6.3.0.p0.1279813&#x2F;lib&#x2F;hive&#x2F;lib&#x2F;hive-serde-2.1.1-cdh6.3.0.jar</span><br><span class="line">&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.3.0-1.cdh6.3.0.p0.1279813&#x2F;lib&#x2F;oozie&#x2F;embedded-oozie-server&#x2F;webapp&#x2F;WEB-INF&#x2F;lib&#x2F;hive-serde-2.1.1-cdh6.3.0.jar</span><br></pre></td></tr></table></figure></div>
<p>有的是符号链接，只需要放入这3个目录：</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;opt&#x2F;cloudera&#x2F;cm&#x2F;common_jars&#x2F;</span><br><span class="line">&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.3.0-1.cdh6.3.0.p0.1279813&#x2F;jars&#x2F;</span><br><span class="line">&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.3.0-1.cdh6.3.0.p0.1279813&#x2F;lib&#x2F;oozie&#x2F;embedded-oozie-server&#x2F;webapp&#x2F;WEB-INF&#x2F;lib&#x2F;</span><br></pre></td></tr></table></figure></div>
<p>后两个目录下jar包文件名直接就是打包出来的jar包名，但是common_jars目录下的jar包文件名中有一串不知道什么算法生成的hash，形如：</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@dev-master2 scripts]# ls -lh &#x2F;opt&#x2F;cloudera&#x2F;cm&#x2F;common_jars&#x2F;hive* </span><br><span class="line">-rw-r--r--. 1 root root  46K Jul 19  2019 &#x2F;opt&#x2F;cloudera&#x2F;cm&#x2F;common_jars&#x2F;hive-ant-2.1.1-cdh6.3.0.f857dabb5222c1969c9f4087c8bfaac3.jar</span><br><span class="line">-rw-r--r--. 1 root root  12K Jul 19  2019 &#x2F;opt&#x2F;cloudera&#x2F;cm&#x2F;common_jars&#x2F;hive-classification-2.1.1-cdh6.3.0.c2ac9c5cf1fbb22aeda542f287ecbaa4.jar</span><br><span class="line">-rw-r--r--. 1 root root  46K Jul 19  2019 &#x2F;opt&#x2F;cloudera&#x2F;cm&#x2F;common_jars&#x2F;hive-cli-2.1.1-cdh6.3.0.f8741782bcbf8b4b58f537da6346e0ff.jar</span><br><span class="line">-rw-r--r--. 1 root root 324K Jul 19  2019 &#x2F;opt&#x2F;cloudera&#x2F;cm&#x2F;common_jars&#x2F;hive-common-1.1.0-cdh5.12.0.10beb989e3d6a390afce045b1e865bde.jar</span><br><span class="line">-rw-r--r--. 1 root root 429K Jul 19  2019 &#x2F;opt&#x2F;cloudera&#x2F;cm&#x2F;common_jars&#x2F;hive-common-2.1.1-cdh6.3.0.87dadce3138dc2c5c2e696cc6f6f7927.jar</span><br><span class="line">......</span><br></pre></td></tr></table></figure></div>
<p>以前替换yarn一个有并发修改问题的jar包也遇到这种情况，但是jar包替换后使用原来一样的带hash的文件名，也没有报校验失败的错误。</p>
<p>所以可将以下脚本与所有打包后lib目录下的hive*.jar放在一个目录下，将目录拷到CDH上每一台（通过一些脚本）机器上，并在每一台（通过一些脚本）机器上运行该脚本，替换hive的jar包，然后<font color="red"><b>重启Hive</b></font>即可。</p>
<p>copy_jars.sh（带hash的那部分可从机器上拷出来，再通过正则替换生成）</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">shell</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/usr/bin/env bash</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 参考/user/bin/hive脚本：Reference: http://stackoverflow.com/questions/59895/can-a-bash-script-tell-what-directory-its-stored-in</span></span><br><span class="line">current_dir=$( cd -- &quot;$( dirname -- &quot;$&#123;BASH_SOURCE[0]&#125;&quot; )&quot; &amp;&gt; /dev/null &amp;&amp; pwd )</span><br><span class="line">echo &quot;current_dir: $current_dir&quot;</span><br><span class="line">cp $current_dir/hive*.jar /opt/cloudera/parcels/CDH-6.3.0-1.cdh6.3.0.p0.1279813/jars/</span><br><span class="line">cp $current_dir/hive*.jar /opt/cloudera/parcels/CDH-6.3.0-1.cdh6.3.0.p0.1279813/lib/oozie/embedded-oozie-server/webapp/WEB-INF/lib/</span><br><span class="line"></span><br><span class="line">cp $current_dir/hive-ant-2.1.1-cdh6.3.0.jar /opt/cloudera/cm/common_jars/hive-ant-2.1.1-cdh6.3.0.f857dabb5222c1969c9f4087c8bfaac3.jar</span><br><span class="line">cp $current_dir/hive-classification-2.1.1-cdh6.3.0.jar /opt/cloudera/cm/common_jars/hive-classification-2.1.1-cdh6.3.0.c2ac9c5cf1fbb22aeda542f287ecbaa4.jar</span><br><span class="line">cp $current_dir/hive-cli-2.1.1-cdh6.3.0.jar /opt/cloudera/cm/common_jars/hive-cli-2.1.1-cdh6.3.0.f8741782bcbf8b4b58f537da6346e0ff.jar</span><br><span class="line">cp $current_dir/hive-common-1.1.0-cdh5.12.0.jar /opt/cloudera/cm/common_jars/hive-common-1.1.0-cdh5.12.0.10beb989e3d6a390afce045b1e865bde.jar</span><br><span class="line">cp $current_dir/hive-common-2.1.1-cdh6.3.0.jar /opt/cloudera/cm/common_jars/hive-common-2.1.1-cdh6.3.0.87dadce3138dc2c5c2e696cc6f6f7927.jar</span><br><span class="line">cp $current_dir/hive-exec-2.1.1-cdh6.3.0.jar /opt/cloudera/cm/common_jars/hive-exec-2.1.1-cdh6.3.0.15d37ff81bca70d35b904a6946abea49.jar</span><br><span class="line">cp $current_dir/hive-jdbc-2.1.1-cdh6.3.0.jar /opt/cloudera/cm/common_jars/hive-jdbc-2.1.1-cdh6.3.0.a9016068a26246ac47c4b2637db33adb.jar</span><br><span class="line">cp $current_dir/hive-llap-client-2.1.1-cdh6.3.0.jar /opt/cloudera/cm/common_jars/hive-llap-client-2.1.1-cdh6.3.0.701f1dfc66958f0d8feab78d602b9cb6.jar</span><br><span class="line">cp $current_dir/hive-llap-common-2.1.1-cdh6.3.0.jar /opt/cloudera/cm/common_jars/hive-llap-common-2.1.1-cdh6.3.0.6c733dcdfa1e52ce79dc1b0066220a00.jar</span><br><span class="line">cp $current_dir/hive-llap-server-2.1.1-cdh6.3.0.jar /opt/cloudera/cm/common_jars/hive-llap-server-2.1.1-cdh6.3.0.105d9633082dfb213b9d390dc3df8087.jar</span><br><span class="line">cp $current_dir/hive-llap-tez-2.1.1-cdh6.3.0.jar /opt/cloudera/cm/common_jars/hive-llap-tez-2.1.1-cdh6.3.0.47ac2463acf7de1929b57c4da5ac7f41.jar</span><br><span class="line">cp $current_dir/hive-metastore-1.1.0-cdh5.12.0.jar /opt/cloudera/cm/common_jars/hive-metastore-1.1.0-cdh5.12.0.f439e1b26177542bfc57e428717a265a.jar</span><br><span class="line">cp $current_dir/hive-metastore-2.1.1-cdh6.3.0.jar /opt/cloudera/cm/common_jars/hive-metastore-2.1.1-cdh6.3.0.4a407e44f9168f014f41edd4a56d5028.jar</span><br><span class="line">cp $current_dir/hive-orc-2.1.1-cdh6.3.0.jar /opt/cloudera/cm/common_jars/hive-orc-2.1.1-cdh6.3.0.0d1f0cf02d1bdad572cca211654c64af.jar</span><br><span class="line">cp $current_dir/hive-serde-1.1.0-cdh5.12.0.jar /opt/cloudera/cm/common_jars/hive-serde-1.1.0-cdh5.12.0.62c4570f4681c0698b9f5f5ab6baab4a.jar</span><br><span class="line">cp $current_dir/hive-serde-2.1.1-cdh6.3.0.jar /opt/cloudera/cm/common_jars/hive-serde-2.1.1-cdh6.3.0.bde9116deea651dbf085034565504351.jar</span><br><span class="line">cp $current_dir/hive-service-2.1.1-cdh6.3.0.jar /opt/cloudera/cm/common_jars/hive-service-2.1.1-cdh6.3.0.0c28a52a856414cb45d0b827bd7884e9.jar</span><br><span class="line">cp $current_dir/hive-service-rpc-2.1.1-cdh6.3.0.jar /opt/cloudera/cm/common_jars/hive-service-rpc-2.1.1-cdh6.3.0.fde12a48a558128e4d15bfb47f90cfb4.jar</span><br><span class="line">cp $current_dir/hive-shims-1.1.0-cdh5.12.0.jar /opt/cloudera/cm/common_jars/hive-shims-1.1.0-cdh5.12.0.2698b9ffda7580409fc299d986f41ded.jar</span><br><span class="line">cp $current_dir/hive-shims-2.1.1-cdh6.3.0.jar /opt/cloudera/cm/common_jars/hive-shims-2.1.1-cdh6.3.0.a151f9e3d14dfeb5bb2b34e0b2ef8a28.jar</span><br><span class="line">cp $current_dir/hive-storage-api-2.1.1-cdh6.3.0.jar /opt/cloudera/cm/common_jars/hive-storage-api-2.1.1-cdh6.3.0.fb98d759511d27287bcd20e48b40f961.jar</span><br></pre></td></tr></table></figure></div>

<h1><span id="五-可能的其他方案">五、可能的其他方案？</span></h1><ul>
<li>如从Hive表的INPUTFORMAT切入</li>
<li>更加熟悉Hive流程后，看其他地方是否能更方便地处理或全局处理</li>
<li>有空看看Hive3的兼容怎么做的，“学习学习”</li>
</ul>
<p><a href="/img/bg-images/nengyongjiuxing.jpg" data-fancybox="group" data-caption="能用就行" class="fancybox"><img alt="能用就行" title="能用就行" data-src="/img/bg-images/nengyongjiuxing.jpg" class="lazyload"></a></p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">0x3E6</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://longwang.live/2022/03/08/%E4%B8%80%E7%A7%8D%E5%A4%84%E7%90%86Hive%E5%85%83%E6%95%B0%E6%8D%AE%E4%B8%8E%E6%96%87%E4%BB%B6%E7%B1%BB%E5%9E%8B%E4%B8%8D%E5%90%8C%E6%97%B6SQL%E6%9F%A5%E8%AF%A2%E5%A4%B1%E8%B4%A5%E7%9A%84%E6%96%B9%E6%B3%95/">http://longwang.live/2022/03/08/%E4%B8%80%E7%A7%8D%E5%A4%84%E7%90%86Hive%E5%85%83%E6%95%B0%E6%8D%AE%E4%B8%8E%E6%96%87%E4%BB%B6%E7%B1%BB%E5%9E%8B%E4%B8%8D%E5%90%8C%E6%97%B6SQL%E6%9F%A5%E8%AF%A2%E5%A4%B1%E8%B4%A5%E7%9A%84%E6%96%B9%E6%B3%95/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://longwang.live">0x3E6的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据    </a><a class="post-meta__tags" href="/tags/Hive/">Hive    </a></div><div class="post_share"><div class="social-share" data-image="img/bg-images/Agatsuma Zenitsu.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-button button--primary button--animated"> <i class="fa fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/wechat.jpg" alt="微信"><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/alipay.jpg" alt="支付寶"><div class="post-qr-code__desc">支付寶</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="prev-post pull_left"><a href="/2022/03/24/%E4%B8%80%E7%A7%8D%E5%A4%84%E7%90%86Hive%E5%85%83%E6%95%B0%E6%8D%AE%E4%B8%8E%E6%96%87%E4%BB%B6%E7%B1%BB%E5%9E%8B%E4%B8%8D%E5%90%8C%E6%97%B6SQL%E6%9F%A5%E8%AF%A2%E5%A4%B1%E8%B4%A5%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%88%E4%BA%8C%EF%BC%89/"><img class="prev_cover lazyload" data-src="img/bg-images/fanzhou.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="label">上一篇</div><div class="prev_info"><span>一种处理Hive元数据与文件类型不同时SQL查询失败的方法（二）</span></div></a></div><div class="next-post pull_right"><a href="/2021/03/23/Hive%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95/"><img class="next_cover lazyload" data-src="img/AttackOnTitan/进击的巨人.png" onerror="onerror=null;src='/img/404.jpg'"><div class="label">下一篇</div><div class="next_info"><span>Hive源码调试</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2021/03/23/Hive源码调试/" title="Hive源码调试"><img class="relatedPosts_cover lazyload"data-src="img/AttackOnTitan/进击的巨人.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2021-03-23</div><div class="relatedPosts_title">Hive源码调试</div></div></a></div><div class="relatedPosts_item"><a href="/2022/03/24/一种处理Hive元数据与文件类型不同时SQL查询失败的方法（二）/" title="一种处理Hive元数据与文件类型不同时SQL查询失败的方法（二）"><img class="relatedPosts_cover lazyload"data-src="img/bg-images/fanzhou.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2022-03-24</div><div class="relatedPosts_title">一种处理Hive元数据与文件类型不同时SQL查询失败的方法（二）</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/25/一种动态更新flink任务配置的方法/" title="一种动态更新flink任务配置的方法"><img class="relatedPosts_cover lazyload"data-src="img/laopo/sznk1.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-25</div><div class="relatedPosts_title">一种动态更新flink任务配置的方法</div></div></a></div><div class="relatedPosts_item"><a href="/2020/02/21/一种处理Sqoop导出过程中数据的方法/" title="一种处理Sqoop导出过程中数据的方法"><img class="relatedPosts_cover lazyload"data-src="img/laopo/misaka-mikoto-1.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-02-21</div><div class="relatedPosts_title">一种处理Sqoop导出过程中数据的方法</div></div></a></div></div><div class="clear_both"></div></div></div></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2022 By 0x3E6</div><div class="framework-info"><span>驱动 </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly"><span>Butterfly</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换" target="_self">繁</a><i class="darkmode fa fa-moon-o" id="darkmode" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script></body></html>