<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>一种处理Hive元数据与文件类型不同时SQL查询失败的方法（二） | 0x3E6的博客</title><meta name="description" content="继上一篇之后，又发现了一种新的报错位置。本篇对这种情况进行处理，并验证这种处理方式是否适用于Hive on Spark环境。"><meta name="keywords" content="大数据,Hive"><meta name="author" content="0x3E6"><meta name="copyright" content="0x3E6"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin><link rel="preconnect" href="//busuanzi.ibruce.info"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="一种处理Hive元数据与文件类型不同时SQL查询失败的方法（二）"><meta name="twitter:description" content="继上一篇之后，又发现了一种新的报错位置。本篇对这种情况进行处理，并验证这种处理方式是否适用于Hive on Spark环境。"><meta name="twitter:image" content="http://longwang.liveimg/bg-images/fanzhou.jpg"><meta property="og:type" content="article"><meta property="og:title" content="一种处理Hive元数据与文件类型不同时SQL查询失败的方法（二）"><meta property="og:url" content="http://longwang.live/2022/03/24/%E4%B8%80%E7%A7%8D%E5%A4%84%E7%90%86Hive%E5%85%83%E6%95%B0%E6%8D%AE%E4%B8%8E%E6%96%87%E4%BB%B6%E7%B1%BB%E5%9E%8B%E4%B8%8D%E5%90%8C%E6%97%B6SQL%E6%9F%A5%E8%AF%A2%E5%A4%B1%E8%B4%A5%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%88%E4%BA%8C%EF%BC%89/"><meta property="og:site_name" content="0x3E6的博客"><meta property="og:description" content="继上一篇之后，又发现了一种新的报错位置。本篇对这种情况进行处理，并验证这种处理方式是否适用于Hive on Spark环境。"><meta property="og:image" content="http://longwang.liveimg/bg-images/fanzhou.jpg"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>const autoChangeMode = 'false'
var t = Cookies.get("theme");
if (autoChangeMode == '1'){
const isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
const isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
const isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

if (t === undefined){
  if (isLightMode) activateLightMode()
  else if (isDarkMode) activateDarkMode()
  else if (isNotSpecified || hasNoSupport){
    console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
    now = new Date();
    hour = now.getHours();
    isNight = hour < 6 || hour >= 18
    isNight ? activateDarkMode() : activateLightMode()
}
} else if (t == 'light') activateLightMode()
else activateDarkMode()


} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="canonical" href="http://longwang.live/2022/03/24/%E4%B8%80%E7%A7%8D%E5%A4%84%E7%90%86Hive%E5%85%83%E6%95%B0%E6%8D%AE%E4%B8%8E%E6%96%87%E4%BB%B6%E7%B1%BB%E5%9E%8B%E4%B8%8D%E5%90%8C%E6%97%B6SQL%E6%9F%A5%E8%AF%A2%E5%A4%B1%E8%B4%A5%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%88%E4%BA%8C%EF%BC%89/"><link rel="next" title="一种处理Hive元数据与文件类型不同时SQL查询失败的方法" href="http://longwang.live/2022/03/08/%E4%B8%80%E7%A7%8D%E5%A4%84%E7%90%86Hive%E5%85%83%E6%95%B0%E6%8D%AE%E4%B8%8E%E6%96%87%E4%BB%B6%E7%B1%BB%E5%9E%8B%E4%B8%8D%E5%90%8C%E6%97%B6SQL%E6%9F%A5%E8%AF%A2%E5%A4%B1%E8%B4%A5%E7%9A%84%E6%96%B9%E6%B3%95/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    title: 'Snackbar.bookmark.title',
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  baiduPush: false,
  isHome: false,
  isPost: true
  
}</script><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="0x3E6的博客" type="application/atom+xml">
</head><body><header> <div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">0x3E6的博客</a></span><span class="toggle-menu pull_right close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-list" aria-hidden="true"></i><span> List</span><i class="fa fa-chevron-down menus-expand" aria-hidden="true"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fa fa-music"></i><span> Music</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fa fa-film"></i><span> Movie</span></a></li></ul></div></div></span></div></header><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">6</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">5</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-list" aria-hidden="true"></i><span> List</span><i class="fa fa-chevron-down menus-expand" aria-hidden="true"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fa fa-music"></i><span> Music</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fa fa-film"></i><span> Movie</span></a></li></ul></div></div></div><div id="mobile-sidebar-toc"><div class="toc_mobile_headline">目录</div><div class="sidebar-toc__content"><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text">一、异常触发SQL</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link"><span class="toc_mobile_items-number">2.</span> <span class="toc_mobile_items-text">二、异常处理</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link"><span class="toc_mobile_items-number">3.</span> <span class="toc_mobile_items-text">三、Hive on Spark依赖的Hive jar包部署</span></a></li></ol></div></div></div><div id="body-wrap"><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true">     </i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">一、异常触发SQL</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">二、异常处理</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">3.</span> <span class="toc-text">三、Hive on Spark依赖的Hive jar包部署</span></a></li></ol></div></div></div><main id="content-outer"><div id="top-container" style="background-image: url(img/bg-images/fanzhou.jpg)"><div id="post-info"><div id="post-title"><div class="posttitle">一种处理Hive元数据与文件类型不同时SQL查询失败的方法（二）</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 发表于 2022-03-24<span class="post-meta__separator">|</span><i class="fa fa-history fa-fw" aria-hidden="true"></i> 更新于 2022-03-26</time><div class="post-meta-wordcount"><div class="post-meta-pv-cv"><span><i class="fa fa-eye post-meta__icon fa-fw" aria-hidden="true"> </i>阅读量:</span><span id="busuanzi_value_page_pv"></span></div></div></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><p>继上一篇之后，又发现了一种新的报错位置，本篇对这种情况进行处理，并验证这种处理方式是否适用于Hive on Spark环境。</p>
<h1><span id="一-异常触发sql">一、异常触发SQL</span></h1><p>构造测试数据<br>(1) 建表，插入数据</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">sql</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> t1(id <span class="type">float</span>,content string) stored <span class="keyword">as</span> parquet;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> t1 vlaues(<span class="number">1.1</span>,<span class="string">&#x27;content1&#x27;</span>),(<span class="number">2.2</span>,<span class="string">&#x27;content2&#x27;</span>);</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> error_type(id <span class="type">int</span>,content string) stored <span class="keyword">as</span> parquet;</span><br></pre></td></tr></table></figure></div>
<p>(2) 拷贝文件到类型不兼容的表</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -cp &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;testdb.db&#x2F;t1&#x2F;000000_0 &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;testdb.db&#x2F;error_type&#x2F;</span><br></pre></td></tr></table></figure></div>

<p>在前面两步之后，执行如下SQL：</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">sql</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> error_type <span class="keyword">where</span> content<span class="operator">=</span><span class="string">&#x27;content1&#x27;</span>;</span><br></pre></td></tr></table></figure></div>
<p>报错并有如下错误日志：</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line">Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row [Error getting row data with exception java.lang.ClassCastException: org.apache.hadoop.io.FloatWritable cannot be cast to org.apache.hadoop.io.IntWritable</span><br><span class="line">	at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableIntObjectInspector.get(WritableIntObjectInspector.java:36)</span><br><span class="line">	at org.apache.hadoop.hive.serde2.SerDeUtils.buildJSONString(SerDeUtils.java:227)</span><br><span class="line">	at org.apache.hadoop.hive.serde2.SerDeUtils.buildJSONString(SerDeUtils.java:364)</span><br><span class="line">	at org.apache.hadoop.hive.serde2.SerDeUtils.getJSONString(SerDeUtils.java:200)</span><br><span class="line">	at org.apache.hadoop.hive.serde2.SerDeUtils.getJSONString(SerDeUtils.java:186)</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.MapOperator.toErrorMessage(MapOperator.java:520)</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:489)</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.processRow(SparkMapRecordHandler.java:133)</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:48)</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:27)</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList.hasNext(HiveBaseFunctionResultList.java:85)</span><br><span class="line">	at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42)</span><br><span class="line">	at scala.collection.Iterator$class.foreach(Iterator.scala:891)</span><br><span class="line">	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)</span><br><span class="line">	at org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1$$anonfun$apply$12.apply(AsyncRDDActions.scala:127)</span><br><span class="line">	at org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1$$anonfun$apply$12.apply(AsyncRDDActions.scala:127)</span><br><span class="line">	at org.apache.spark.SparkContext$$anonfun$38.apply(SparkContext.scala:2232)</span><br><span class="line">	at org.apache.spark.SparkContext$$anonfun$38.apply(SparkContext.scala:2232)</span><br><span class="line">	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)</span><br><span class="line">	at org.apache.spark.scheduler.Task.run(Task.scala:121)</span><br><span class="line">	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$11.apply(Executor.scala:407)</span><br><span class="line">	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1408)</span><br><span class="line">	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:413)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class="line">	at java.lang.Thread.run(Thread.java:748)</span><br><span class="line"> ]</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:494) ~[hive-exec-2.1.1-cdh6.3.0.jar:2.1.1-cdh6.3.0]</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.processRow(SparkMapRecordHandler.java:133) ~[hive-exec-2.1.1-cdh6.3.0.jar:2.1.1-cdh6.3.0]</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:48) ~[hive-exec-2.1.1-cdh6.3.0.jar:2.1.1-cdh6.3.0]</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:27) ~[hive-exec-2.1.1-cdh6.3.0.jar:2.1.1-cdh6.3.0]</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList.hasNext(HiveBaseFunctionResultList.java:85) ~[hive-exec-2.1.1-cdh6.3.0.jar:2.1.1-cdh6.3.0]</span><br><span class="line">	at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42) ~[scala-library-2.11.12.jar:?]</span><br><span class="line">	at scala.collection.Iterator$class.foreach(Iterator.scala:891) ~[scala-library-2.11.12.jar:?]</span><br><span class="line">	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) ~[scala-library-2.11.12.jar:?]</span><br><span class="line">	at org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1$$anonfun$apply$12.apply(AsyncRDDActions.scala:127) ~[spark-core_2.11-2.4.0-cdh6.3.0.jar:2.4.0-cdh6.3.0]</span><br><span class="line">	at org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1$$anonfun$apply$12.apply(AsyncRDDActions.scala:127) ~[spark-core_2.11-2.4.0-cdh6.3.0.jar:2.4.0-cdh6.3.0]</span><br><span class="line">	at org.apache.spark.SparkContext$$anonfun$38.apply(SparkContext.scala:2232) ~[spark-core_2.11-2.4.0-cdh6.3.0.jar:2.4.0-cdh6.3.0]</span><br><span class="line">	at org.apache.spark.SparkContext$$anonfun$38.apply(SparkContext.scala:2232) ~[spark-core_2.11-2.4.0-cdh6.3.0.jar:2.4.0-cdh6.3.0]</span><br><span class="line">	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.11-2.4.0-cdh6.3.0.jar:2.4.0-cdh6.3.0]</span><br><span class="line">	at org.apache.spark.scheduler.Task.run(Task.scala:121) ~[spark-core_2.11-2.4.0-cdh6.3.0.jar:2.4.0-cdh6.3.0]</span><br><span class="line">	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$11.apply(Executor.scala:407) ~[spark-core_2.11-2.4.0-cdh6.3.0.jar:2.4.0-cdh6.3.0]</span><br><span class="line">	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1408) ~[spark-core_2.11-2.4.0-cdh6.3.0.jar:2.4.0-cdh6.3.0]</span><br><span class="line">	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:413) ~[spark-core_2.11-2.4.0-cdh6.3.0.jar:2.4.0-cdh6.3.0]</span><br><span class="line">	... 3 more</span><br><span class="line">Caused by: java.lang.ClassCastException: org.apache.hadoop.io.FloatWritable cannot be cast to org.apache.hadoop.io.IntWritable</span><br><span class="line">	at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableIntObjectInspector.get(WritableIntObjectInspector.java:36) ~[hive-exec-2.1.1-cdh6.3.0.jar:2.1.1-cdh6.3.0]</span><br><span class="line">	at org.apache.hadoop.hive.serde2.lazy.LazyUtils.writePrimitiveUTF8(LazyUtils.java:251) ~[hive-exec-2.1.1-cdh6.3.0.jar:2.1.1-cdh6.3.0]</span><br><span class="line">	at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:292) ~[hive-exec-2.1.1-cdh6.3.0.jar:2.1.1-cdh6.3.0]</span><br><span class="line">	at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serializeField(LazySimpleSerDe.java:247) ~[hive-exec-2.1.1-cdh6.3.0.jar:2.1.1-cdh6.3.0]</span><br><span class="line">	at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.doSerialize(LazySimpleSerDe.java:231) ~[hive-exec-2.1.1-cdh6.3.0.jar:2.1.1-cdh6.3.0]</span><br><span class="line">	at org.apache.hadoop.hive.serde2.AbstractEncodingAwareSerDe.serialize(AbstractEncodingAwareSerDe.java:55) ~[hive-exec-2.1.1-cdh6.3.0.jar:2.1.1-cdh6.3.0]</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.process(FileSinkOperator.java:732) ~[hive-exec-2.1.1-cdh6.3.0.jar:2.1.1-cdh6.3.0]</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:882) ~[hive-exec-2.1.1-cdh6.3.0.jar:2.1.1-cdh6.3.0]</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:95) ~[hive-exec-2.1.1-cdh6.3.0.jar:2.1.1-cdh6.3.0]</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:882) ~[hive-exec-2.1.1-cdh6.3.0.jar:2.1.1-cdh6.3.0]</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.FilterOperator.process(FilterOperator.java:126) ~[hive-exec-2.1.1-cdh6.3.0.jar:2.1.1-cdh6.3.0]</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:882) ~[hive-exec-2.1.1-cdh6.3.0.jar:2.1.1-cdh6.3.0]</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:130) ~[hive-exec-2.1.1-cdh6.3.0.jar:2.1.1-cdh6.3.0]</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:146) ~[hive-exec-2.1.1-cdh6.3.0.jar:2.1.1-cdh6.3.0]</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:484) ~[hive-exec-2.1.1-cdh6.3.0.jar:2.1.1-cdh6.3.0]</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.processRow(SparkMapRecordHandler.java:133) ~[hive-exec-2.1.1-cdh6.3.0.jar:2.1.1-cdh6.3.0]</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:48) ~[hive-exec-2.1.1-cdh6.3.0.jar:2.1.1-cdh6.3.0]</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:27) ~[hive-exec-2.1.1-cdh6.3.0.jar:2.1.1-cdh6.3.0]</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList.hasNext(HiveBaseFunctionResultList.java:85) ~[hive-exec-2.1.1-cdh6.3.0.jar:2.1.1-cdh6.3.0]</span><br><span class="line">	at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42) ~[scala-library-2.11.12.jar:?]</span><br><span class="line">	at scala.collection.Iterator$class.foreach(Iterator.scala:891) ~[scala-library-2.11.12.jar:?]</span><br><span class="line">	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) ~[scala-library-2.11.12.jar:?]</span><br><span class="line">	at org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1$$anonfun$apply$12.apply(AsyncRDDActions.scala:127) ~[spark-core_2.11-2.4.0-cdh6.3.0.jar:2.4.0-cdh6.3.0]</span><br><span class="line">	at org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1$$anonfun$apply$12.apply(AsyncRDDActions.scala:127) ~[spark-core_2.11-2.4.0-cdh6.3.0.jar:2.4.0-cdh6.3.0]</span><br><span class="line">	at org.apache.spark.SparkContext$$anonfun$38.apply(SparkContext.scala:2232) ~[spark-core_2.11-2.4.0-cdh6.3.0.jar:2.4.0-cdh6.3.0]</span><br><span class="line">	at org.apache.spark.SparkContext$$anonfun$38.apply(SparkContext.scala:2232) ~[spark-core_2.11-2.4.0-cdh6.3.0.jar:2.4.0-cdh6.3.0]</span><br><span class="line">	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.11-2.4.0-cdh6.3.0.jar:2.4.0-cdh6.3.0]</span><br><span class="line">	at org.apache.spark.scheduler.Task.run(Task.scala:121) ~[spark-core_2.11-2.4.0-cdh6.3.0.jar:2.4.0-cdh6.3.0]</span><br><span class="line">	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$11.apply(Executor.scala:407) ~[spark-core_2.11-2.4.0-cdh6.3.0.jar:2.4.0-cdh6.3.0]</span><br><span class="line">	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1408) ~[spark-core_2.11-2.4.0-cdh6.3.0.jar:2.4.0-cdh6.3.0]</span><br><span class="line">	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:413) ~[spark-core_2.11-2.4.0-cdh6.3.0.jar:2.4.0-cdh6.3.0]</span><br><span class="line">	... 3 more</span><br></pre></td></tr></table></figure></div>
<h1><span id="二-异常处理">二、异常处理</span></h1><p>其中<code>org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.doSerialize(LazySimpleSerDe.java:231)</code>函数中有序列化每个字段的逻辑：</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Serialize a row of data.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> obj</span></span><br><span class="line"><span class="comment"> *          The row object</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> objInspector</span></span><br><span class="line"><span class="comment"> *          The ObjectInspector for the row object</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> The serialized Writable object</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@see</span> SerDe#serialize(Object, ObjectInspector)</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Writable <span class="title">doSerialize</span><span class="params">(Object obj, ObjectInspector objInspector)</span></span></span><br><span class="line"><span class="function">    <span class="keyword">throws</span> SerDeException </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (objInspector.getCategory() != Category.STRUCT) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> SerDeException(getClass().toString()</span><br><span class="line">        + <span class="string">&quot; can only serialize struct types, but we got: &quot;</span></span><br><span class="line">        + objInspector.getTypeName());</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Prepare the field ObjectInspectors</span></span><br><span class="line">  StructObjectInspector soi = (StructObjectInspector) objInspector;</span><br><span class="line">  List&lt;? extends StructField&gt; fields = soi.getAllStructFieldRefs();</span><br><span class="line">  List&lt;Object&gt; list = soi.getStructFieldsDataAsList(obj);</span><br><span class="line">  List&lt;? extends StructField&gt; declaredFields = (serdeParams.getRowTypeInfo() != <span class="keyword">null</span> &amp;&amp; ((StructTypeInfo) serdeParams.getRowTypeInfo())</span><br><span class="line">      .getAllStructFieldNames().size() &gt; <span class="number">0</span>) ? ((StructObjectInspector) getObjectInspector())</span><br><span class="line">      .getAllStructFieldRefs()</span><br><span class="line">      : <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">  serializeStream.reset();</span><br><span class="line">  serializedSize = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Serialize each field</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; fields.size(); i++) &#123;</span><br><span class="line">    <span class="comment">// Append the separator if needed.</span></span><br><span class="line">    <span class="keyword">if</span> (i &gt; <span class="number">0</span>) &#123;</span><br><span class="line">      serializeStream.write(serdeParams.getSeparators()[<span class="number">0</span>]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// Get the field objectInspector and the field object.</span></span><br><span class="line">    ObjectInspector foi = fields.get(i).getFieldObjectInspector();</span><br><span class="line">    Object f = (list == <span class="keyword">null</span> ? <span class="keyword">null</span> : list.get(i));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (declaredFields != <span class="keyword">null</span> &amp;&amp; i &gt;= declaredFields.size()) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> SerDeException(<span class="string">&quot;Error: expecting &quot;</span> + declaredFields.size()</span><br><span class="line">          + <span class="string">&quot; but asking for field &quot;</span> + i + <span class="string">&quot;\n&quot;</span> + <span class="string">&quot;data=&quot;</span> + obj + <span class="string">&quot;\n&quot;</span></span><br><span class="line">          + <span class="string">&quot;tableType=&quot;</span> + serdeParams.getRowTypeInfo().toString() + <span class="string">&quot;\n&quot;</span></span><br><span class="line">          + <span class="string">&quot;dataType=&quot;</span></span><br><span class="line">          + TypeInfoUtils.getTypeInfoFromObjectInspector(objInspector));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    serializeField(serializeStream, f, foi, serdeParams);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// <span class="doctag">TODO:</span> The copy of data is unnecessary, but there is no work-around</span></span><br><span class="line">  <span class="comment">// since we cannot directly set the private byte[] field inside Text.</span></span><br><span class="line">  serializeCache</span><br><span class="line">      .set(serializeStream.getData(), <span class="number">0</span>, serializeStream.getLength());</span><br><span class="line">  serializedSize = serializeStream.getLength();</span><br><span class="line">  lastOperationSerialize = <span class="keyword">true</span>;</span><br><span class="line">  lastOperationDeserialize = <span class="keyword">false</span>;</span><br><span class="line">  <span class="keyword">return</span> serializeCache;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
<p>for循环中的最后一行<code>serializeField(serializeStream, f, foi, serdeParams);</code>调用的即是异常堆栈中的<code>org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serializeField(LazySimpleSerDe.java:247)</code>函数：</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">serializeField</span><span class="params">(ByteStream.Output out, Object obj, ObjectInspector objInspector,</span></span></span><br><span class="line"><span class="function"><span class="params">    LazySerDeParameters serdeParams)</span> <span class="keyword">throws</span> SerDeException </span>&#123;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    serialize(out, obj, objInspector, serdeParams.getSeparators(), <span class="number">1</span>, serdeParams.getNullSequence(),</span><br><span class="line">        serdeParams.isEscaped(), serdeParams.getEscapeChar(), serdeParams.getNeedsEscape());</span><br><span class="line">  &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> SerDeException(e);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
<p>serializeField中调用的serialize函数为异常堆栈中的<code>org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:292)</code>函数：</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Serialize the row into the StringBuilder.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> out</span></span><br><span class="line"><span class="comment"> *          The StringBuilder to store the serialized data.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> obj</span></span><br><span class="line"><span class="comment"> *          The object for the current field.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> objInspector</span></span><br><span class="line"><span class="comment"> *          The ObjectInspector for the current Object.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> separators</span></span><br><span class="line"><span class="comment"> *          The separators array.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> level</span></span><br><span class="line"><span class="comment"> *          The current level of separator.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> nullSequence</span></span><br><span class="line"><span class="comment"> *          The byte sequence representing the NULL value.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> escaped</span></span><br><span class="line"><span class="comment"> *          Whether we need to escape the data when writing out</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> escapeChar</span></span><br><span class="line"><span class="comment"> *          Which char to use as the escape char, e.g. &#x27;\\&#x27;</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> needsEscape</span></span><br><span class="line"><span class="comment"> *          Which byte needs to be escaped for 256 bytes. </span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> SerDeException</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">serialize</span><span class="params">(ByteStream.Output out, Object obj,</span></span></span><br><span class="line"><span class="function"><span class="params">    ObjectInspector objInspector, <span class="keyword">byte</span>[] separators, <span class="keyword">int</span> level,</span></span></span><br><span class="line"><span class="function"><span class="params">    Text nullSequence, <span class="keyword">boolean</span> escaped, <span class="keyword">byte</span> escapeChar, <span class="keyword">boolean</span>[] needsEscape)</span></span></span><br><span class="line"><span class="function">    <span class="keyword">throws</span> IOException, SerDeException </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (obj == <span class="keyword">null</span>) &#123;</span><br><span class="line">    out.write(nullSequence.getBytes(), <span class="number">0</span>, nullSequence.getLength());</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">char</span> separator;</span><br><span class="line">  List&lt;?&gt; list;</span><br><span class="line">  <span class="keyword">switch</span> (objInspector.getCategory()) &#123;</span><br><span class="line">  <span class="keyword">case</span> PRIMITIVE:</span><br><span class="line">    LazyUtils.writePrimitiveUTF8(out, obj,</span><br><span class="line">        (PrimitiveObjectInspector) objInspector, escaped, escapeChar,</span><br><span class="line">        needsEscape);</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  <span class="keyword">case</span> LIST:</span><br><span class="line">    ......</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  <span class="keyword">case</span> MAP:</span><br><span class="line">    ......</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  <span class="keyword">case</span> STRUCT:</span><br><span class="line">    ......</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  <span class="keyword">case</span> UNION:</span><br><span class="line">    ......</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  <span class="keyword">default</span>:</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">&quot;Unknown category type: &quot;</span></span><br><span class="line">      + objInspector.getCategory());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
<p>在这个函数中，调用后续针对特定类型的函数对特定类型进行序列化，类型不兼容时则抛出异常。可以看到当前字段数据为空时，有如下逻辑：</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (obj == <span class="keyword">null</span>) &#123;</span><br><span class="line">      out.write(nullSequence.getBytes(), <span class="number">0</span>, nullSequence.getLength());</span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></div>
<p>所以还是可以在<code>LazySimpleSerDe.doSerialize</code>函数中处理每个字段的逻辑中，捕获ClassCastException，并参考serialize函数这种逻辑写入空值，将<code>LazySimpleSerDe.doSerialize</code>中</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">serializeField(serializeStream, f, foi, serdeParams);</span><br></pre></td></tr></table></figure></div>
<p>改成</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    serializeField(serializeStream, f, foi, serdeParams);</span><br><span class="line">&#125; <span class="keyword">catch</span> (ClassCastException | UnsupportedOperationException e) &#123;</span><br><span class="line">    serializeStream.write(serdeParams.getNullSequence().getBytes(), <span class="number">0</span>, serdeParams.getNullSequence().getLength());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
<h1><span id="三-hive-on-spark依赖的hive-jar包部署">三、Hive on Spark依赖的Hive jar包部署</span></h1><p>上面代码修改后，用前一篇文章中的copy_jars.sh脚本将hive*.jar部署后，Hive默认的MR执行引擎已经可以执行本文开始提到的会报错的SQL，但是当Hive使用Spark作为执行引擎时（如beeline中可通过<b>set hive.execution.engine=spark;</b>设置），仍然会报错，猜测Spark使用的Hive依赖包在另外的位置也存放了一份。</p>
<p>从前面的日志可以看出，一部分日志后面都显示了hive-exec-2.1.1-cdh6.3.0.jar这个jar包名，在部署了CDH的主机上搜索这个jar包：</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">shell</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@dev-master2 tmp]# find / -name hive-exec-2.1.1-cdh6.3.0.jar</span><br><span class="line">/opt/cloudera/cm/cloudera-navigator-server/libs/cdh6/hive-exec-2.1.1-cdh6.3.0.jar</span><br><span class="line">/opt/cloudera/cm/cloudera-scm-telepub/libs/cdh6/hive-exec-2.1.1-cdh6.3.0.jar</span><br><span class="line">/opt/cloudera/parcels/CDH-6.3.0-1.cdh6.3.0.p0.1279813/jars/hive-exec-2.1.1-cdh6.3.0.jar</span><br><span class="line">/opt/cloudera/parcels/CDH-6.3.0-1.cdh6.3.0.p0.1279813/lib/spark/hive/hive-exec-2.1.1-cdh6.3.0.jar</span><br><span class="line">/opt/cloudera/parcels/CDH-6.3.0-1.cdh6.3.0.p0.1279813/lib/hive/lib/hive-exec-2.1.1-cdh6.3.0.jar</span><br><span class="line">/opt/cloudera/parcels/CDH-6.3.0-1.cdh6.3.0.p0.1279813/lib/oozie/embedded-oozie-server/webapp/WEB-INF/lib/hive-exec-2.1.1-cdh6.3.0.jar</span><br></pre></td></tr></table></figure></div>
<p>看起来/opt/cloudera/parcels/CDH-6.3.0-1.cdh6.3.0.p0.1279813/lib/spark/hive/hive-exec-2.1.1-cdh6.3.0.jar这个就是Spark使用的Hive依赖包存放位置，且这个目录下只有一个jar包：</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">shell</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@dev-master2 tmp]# ls -lh /opt/cloudera/parcels/CDH-6.3.0-1.cdh6.3.0.p0.1279813/lib/spark/hive/</span><br><span class="line">total 35M</span><br><span class="line">-rw-r--r--. 1 root root 35M Jul 19  2019 hive-exec-2.1.1-cdh6.3.0.jar</span><br></pre></td></tr></table></figure></div>
<p>所以在copy_jars.sh中添加一句</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp $current_dir&#x2F;hive-exec-2.1.1-cdh6.3.0.jar &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.3.0-1.cdh6.3.0.p0.1279813&#x2F;lib&#x2F;spark&#x2F;hive&#x2F;</span><br></pre></td></tr></table></figure></div>
<p>再重新部署，经测试，Hive on Spark已经可以查询类型不兼容的类型，结果显示为空值。</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">0x3E6</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://longwang.live/2022/03/24/%E4%B8%80%E7%A7%8D%E5%A4%84%E7%90%86Hive%E5%85%83%E6%95%B0%E6%8D%AE%E4%B8%8E%E6%96%87%E4%BB%B6%E7%B1%BB%E5%9E%8B%E4%B8%8D%E5%90%8C%E6%97%B6SQL%E6%9F%A5%E8%AF%A2%E5%A4%B1%E8%B4%A5%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%88%E4%BA%8C%EF%BC%89/">http://longwang.live/2022/03/24/%E4%B8%80%E7%A7%8D%E5%A4%84%E7%90%86Hive%E5%85%83%E6%95%B0%E6%8D%AE%E4%B8%8E%E6%96%87%E4%BB%B6%E7%B1%BB%E5%9E%8B%E4%B8%8D%E5%90%8C%E6%97%B6SQL%E6%9F%A5%E8%AF%A2%E5%A4%B1%E8%B4%A5%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%88%E4%BA%8C%EF%BC%89/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://longwang.live">0x3E6的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据    </a><a class="post-meta__tags" href="/tags/Hive/">Hive    </a></div><div class="post_share"><div class="social-share" data-image="img/bg-images/fanzhou.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-button button--primary button--animated"> <i class="fa fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/wechat.jpg" alt="微信"><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/alipay.jpg" alt="支付寶"><div class="post-qr-code__desc">支付寶</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="next-post pull-full"><a href="/2022/03/08/%E4%B8%80%E7%A7%8D%E5%A4%84%E7%90%86Hive%E5%85%83%E6%95%B0%E6%8D%AE%E4%B8%8E%E6%96%87%E4%BB%B6%E7%B1%BB%E5%9E%8B%E4%B8%8D%E5%90%8C%E6%97%B6SQL%E6%9F%A5%E8%AF%A2%E5%A4%B1%E8%B4%A5%E7%9A%84%E6%96%B9%E6%B3%95/"><img class="next_cover lazyload" data-src="img/bg-images/Agatsuma Zenitsu.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="label">下一篇</div><div class="next_info"><span>一种处理Hive元数据与文件类型不同时SQL查询失败的方法</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2021/03/23/Hive源码调试/" title="Hive源码调试"><img class="relatedPosts_cover lazyload"data-src="img/AttackOnTitan/进击的巨人.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2021-03-23</div><div class="relatedPosts_title">Hive源码调试</div></div></a></div><div class="relatedPosts_item"><a href="/2022/03/08/一种处理Hive元数据与文件类型不同时SQL查询失败的方法/" title="一种处理Hive元数据与文件类型不同时SQL查询失败的方法"><img class="relatedPosts_cover lazyload"data-src="img/bg-images/Agatsuma Zenitsu.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2022-03-08</div><div class="relatedPosts_title">一种处理Hive元数据与文件类型不同时SQL查询失败的方法</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/25/一种动态更新flink任务配置的方法/" title="一种动态更新flink任务配置的方法"><img class="relatedPosts_cover lazyload"data-src="img/laopo/sznk1.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-25</div><div class="relatedPosts_title">一种动态更新flink任务配置的方法</div></div></a></div><div class="relatedPosts_item"><a href="/2020/02/21/一种处理Sqoop导出过程中数据的方法/" title="一种处理Sqoop导出过程中数据的方法"><img class="relatedPosts_cover lazyload"data-src="img/laopo/misaka-mikoto-1.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-02-21</div><div class="relatedPosts_title">一种处理Sqoop导出过程中数据的方法</div></div></a></div></div><div class="clear_both"></div></div></div></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2022 By 0x3E6</div><div class="framework-info"><span>驱动 </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly"><span>Butterfly</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换" target="_self">繁</a><i class="darkmode fa fa-moon-o" id="darkmode" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script></body></html>