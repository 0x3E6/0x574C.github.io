<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>0x3E6的博客</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://longwang.live/"/>
  <updated>2022-03-26T10:24:10.499Z</updated>
  <id>http://longwang.live/</id>
  
  <author>
    <name>0x3E6</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>一种处理Hive元数据与文件类型不同时SQL查询失败的方法（二）</title>
    <link href="http://longwang.live/2022/03/24/%E4%B8%80%E7%A7%8D%E5%A4%84%E7%90%86Hive%E5%85%83%E6%95%B0%E6%8D%AE%E4%B8%8E%E6%96%87%E4%BB%B6%E7%B1%BB%E5%9E%8B%E4%B8%8D%E5%90%8C%E6%97%B6SQL%E6%9F%A5%E8%AF%A2%E5%A4%B1%E8%B4%A5%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
    <id>http://longwang.live/2022/03/24/%E4%B8%80%E7%A7%8D%E5%A4%84%E7%90%86Hive%E5%85%83%E6%95%B0%E6%8D%AE%E4%B8%8E%E6%96%87%E4%BB%B6%E7%B1%BB%E5%9E%8B%E4%B8%8D%E5%90%8C%E6%97%B6SQL%E6%9F%A5%E8%AF%A2%E5%A4%B1%E8%B4%A5%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%88%E4%BA%8C%EF%BC%89/</id>
    <published>2022-03-24T02:11:31.000Z</published>
    <updated>2022-03-26T10:24:10.499Z</updated>
    
    <content type="html"><![CDATA[<p>继上一篇之后，又发现了一种新的报错位置，本篇对这种情况进行处理，并验证这种处理方式是否适用于Hive on Spark环境。</p><h1><span id="一-异常触发sql">一、异常触发SQL</span></h1><p>构造测试数据<br>(1) 建表，插入数据</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">sql</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> t1(id <span class="type">float</span>,content string) stored <span class="keyword">as</span> parquet;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> t1 vlaues(<span class="number">1.1</span>,<span class="string">&#x27;content1&#x27;</span>),(<span class="number">2.2</span>,<span class="string">&#x27;content2&#x27;</span>);</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> error_type(id <span class="type">int</span>,content string) stored <span class="keyword">as</span> parquet;</span><br></pre></td></tr></table></figure></div><p>(2) 拷贝文件到类型不兼容的表</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -cp &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;testdb.db&#x2F;t1&#x2F;000000_0 &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;testdb.db&#x2F;error_type&#x2F;</span><br></pre></td></tr></table></figure></div><p>在前面两步之后，执行如下SQL：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">sql</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> error_type <span class="keyword">where</span> content<span class="operator">=</span><span class="string">&#x27;content1&#x27;</span>;</span><br></pre></td></tr></table></figure></div><p>报错并有如下错误日志：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line">Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row [Error getting row data with exception java.lang.ClassCastException: org.apache.hadoop.io.FloatWritable cannot be cast to org.apache.hadoop.io.IntWritable</span><br><span class="line">at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableIntObjectInspector.get(WritableIntObjectInspector.java:36)</span><br><span class="line">at org.apache.hadoop.hive.serde2.SerDeUtils.buildJSONString(SerDeUtils.java:227)</span><br><span class="line">at org.apache.hadoop.hive.serde2.SerDeUtils.buildJSONString(SerDeUtils.java:364)</span><br><span class="line">at org.apache.hadoop.hive.serde2.SerDeUtils.getJSONString(SerDeUtils.java:200)</span><br><span class="line">at org.apache.hadoop.hive.serde2.SerDeUtils.getJSONString(SerDeUtils.java:186)</span><br><span class="line">at org.apache.hadoop.hive.ql.exec.MapOperator.toErrorMessage(MapOperator.java:520)</span><br><span class="line">at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:489)</span><br><span class="line">at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.processRow(SparkMapRecordHandler.java:133)</span><br><span class="line">at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:48)</span><br><span class="line">at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:27)</span><br><span class="line">at org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList.hasNext(HiveBaseFunctionResultList.java:85)</span><br><span class="line">at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42)</span><br><span class="line">at scala.collection.Iterator$class.foreach(Iterator.scala:891)</span><br><span class="line">at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)</span><br><span class="line">at org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1$$anonfun$apply$12.apply(AsyncRDDActions.scala:127)</span><br><span class="line">at org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1$$anonfun$apply$12.apply(AsyncRDDActions.scala:127)</span><br><span class="line">at org.apache.spark.SparkContext$$anonfun$38.apply(SparkContext.scala:2232)</span><br><span class="line">at org.apache.spark.SparkContext$$anonfun$38.apply(SparkContext.scala:2232)</span><br><span class="line">at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)</span><br><span class="line">at org.apache.spark.scheduler.Task.run(Task.scala:121)</span><br><span class="line">at org.apache.spark.executor.Executor$TaskRunner$$anonfun$11.apply(Executor.scala:407)</span><br><span class="line">at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1408)</span><br><span class="line">at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:413)</span><br><span class="line">at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</span><br><span class="line">at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class="line">at java.lang.Thread.run(Thread.java:748)</span><br><span class="line"> ]</span><br><span class="line">at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:494) ~[hive-exec-2.1.1-cdh6.3.0.jar:2.1.1-cdh6.3.0]</span><br><span class="line">at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.processRow(SparkMapRecordHandler.java:133) ~[hive-exec-2.1.1-cdh6.3.0.jar:2.1.1-cdh6.3.0]</span><br><span class="line">at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:48) ~[hive-exec-2.1.1-cdh6.3.0.jar:2.1.1-cdh6.3.0]</span><br><span class="line">at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:27) ~[hive-exec-2.1.1-cdh6.3.0.jar:2.1.1-cdh6.3.0]</span><br><span class="line">at org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList.hasNext(HiveBaseFunctionResultList.java:85) ~[hive-exec-2.1.1-cdh6.3.0.jar:2.1.1-cdh6.3.0]</span><br><span class="line">at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42) ~[scala-library-2.11.12.jar:?]</span><br><span class="line">at scala.collection.Iterator$class.foreach(Iterator.scala:891) ~[scala-library-2.11.12.jar:?]</span><br><span class="line">at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) ~[scala-library-2.11.12.jar:?]</span><br><span class="line">at org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1$$anonfun$apply$12.apply(AsyncRDDActions.scala:127) ~[spark-core_2.11-2.4.0-cdh6.3.0.jar:2.4.0-cdh6.3.0]</span><br><span class="line">at org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1$$anonfun$apply$12.apply(AsyncRDDActions.scala:127) ~[spark-core_2.11-2.4.0-cdh6.3.0.jar:2.4.0-cdh6.3.0]</span><br><span class="line">at org.apache.spark.SparkContext$$anonfun$38.apply(SparkContext.scala:2232) ~[spark-core_2.11-2.4.0-cdh6.3.0.jar:2.4.0-cdh6.3.0]</span><br><span class="line">at org.apache.spark.SparkContext$$anonfun$38.apply(SparkContext.scala:2232) ~[spark-core_2.11-2.4.0-cdh6.3.0.jar:2.4.0-cdh6.3.0]</span><br><span class="line">at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.11-2.4.0-cdh6.3.0.jar:2.4.0-cdh6.3.0]</span><br><span class="line">at org.apache.spark.scheduler.Task.run(Task.scala:121) ~[spark-core_2.11-2.4.0-cdh6.3.0.jar:2.4.0-cdh6.3.0]</span><br><span class="line">at org.apache.spark.executor.Executor$TaskRunner$$anonfun$11.apply(Executor.scala:407) ~[spark-core_2.11-2.4.0-cdh6.3.0.jar:2.4.0-cdh6.3.0]</span><br><span class="line">at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1408) ~[spark-core_2.11-2.4.0-cdh6.3.0.jar:2.4.0-cdh6.3.0]</span><br><span class="line">at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:413) ~[spark-core_2.11-2.4.0-cdh6.3.0.jar:2.4.0-cdh6.3.0]</span><br><span class="line">... 3 more</span><br><span class="line">Caused by: java.lang.ClassCastException: org.apache.hadoop.io.FloatWritable cannot be cast to org.apache.hadoop.io.IntWritable</span><br><span class="line">at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableIntObjectInspector.get(WritableIntObjectInspector.java:36) ~[hive-exec-2.1.1-cdh6.3.0.jar:2.1.1-cdh6.3.0]</span><br><span class="line">at org.apache.hadoop.hive.serde2.lazy.LazyUtils.writePrimitiveUTF8(LazyUtils.java:251) ~[hive-exec-2.1.1-cdh6.3.0.jar:2.1.1-cdh6.3.0]</span><br><span class="line">at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:292) ~[hive-exec-2.1.1-cdh6.3.0.jar:2.1.1-cdh6.3.0]</span><br><span class="line">at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serializeField(LazySimpleSerDe.java:247) ~[hive-exec-2.1.1-cdh6.3.0.jar:2.1.1-cdh6.3.0]</span><br><span class="line">at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.doSerialize(LazySimpleSerDe.java:231) ~[hive-exec-2.1.1-cdh6.3.0.jar:2.1.1-cdh6.3.0]</span><br><span class="line">at org.apache.hadoop.hive.serde2.AbstractEncodingAwareSerDe.serialize(AbstractEncodingAwareSerDe.java:55) ~[hive-exec-2.1.1-cdh6.3.0.jar:2.1.1-cdh6.3.0]</span><br><span class="line">at org.apache.hadoop.hive.ql.exec.FileSinkOperator.process(FileSinkOperator.java:732) ~[hive-exec-2.1.1-cdh6.3.0.jar:2.1.1-cdh6.3.0]</span><br><span class="line">at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:882) ~[hive-exec-2.1.1-cdh6.3.0.jar:2.1.1-cdh6.3.0]</span><br><span class="line">at org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:95) ~[hive-exec-2.1.1-cdh6.3.0.jar:2.1.1-cdh6.3.0]</span><br><span class="line">at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:882) ~[hive-exec-2.1.1-cdh6.3.0.jar:2.1.1-cdh6.3.0]</span><br><span class="line">at org.apache.hadoop.hive.ql.exec.FilterOperator.process(FilterOperator.java:126) ~[hive-exec-2.1.1-cdh6.3.0.jar:2.1.1-cdh6.3.0]</span><br><span class="line">at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:882) ~[hive-exec-2.1.1-cdh6.3.0.jar:2.1.1-cdh6.3.0]</span><br><span class="line">at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:130) ~[hive-exec-2.1.1-cdh6.3.0.jar:2.1.1-cdh6.3.0]</span><br><span class="line">at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:146) ~[hive-exec-2.1.1-cdh6.3.0.jar:2.1.1-cdh6.3.0]</span><br><span class="line">at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:484) ~[hive-exec-2.1.1-cdh6.3.0.jar:2.1.1-cdh6.3.0]</span><br><span class="line">at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.processRow(SparkMapRecordHandler.java:133) ~[hive-exec-2.1.1-cdh6.3.0.jar:2.1.1-cdh6.3.0]</span><br><span class="line">at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:48) ~[hive-exec-2.1.1-cdh6.3.0.jar:2.1.1-cdh6.3.0]</span><br><span class="line">at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:27) ~[hive-exec-2.1.1-cdh6.3.0.jar:2.1.1-cdh6.3.0]</span><br><span class="line">at org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList.hasNext(HiveBaseFunctionResultList.java:85) ~[hive-exec-2.1.1-cdh6.3.0.jar:2.1.1-cdh6.3.0]</span><br><span class="line">at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42) ~[scala-library-2.11.12.jar:?]</span><br><span class="line">at scala.collection.Iterator$class.foreach(Iterator.scala:891) ~[scala-library-2.11.12.jar:?]</span><br><span class="line">at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) ~[scala-library-2.11.12.jar:?]</span><br><span class="line">at org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1$$anonfun$apply$12.apply(AsyncRDDActions.scala:127) ~[spark-core_2.11-2.4.0-cdh6.3.0.jar:2.4.0-cdh6.3.0]</span><br><span class="line">at org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1$$anonfun$apply$12.apply(AsyncRDDActions.scala:127) ~[spark-core_2.11-2.4.0-cdh6.3.0.jar:2.4.0-cdh6.3.0]</span><br><span class="line">at org.apache.spark.SparkContext$$anonfun$38.apply(SparkContext.scala:2232) ~[spark-core_2.11-2.4.0-cdh6.3.0.jar:2.4.0-cdh6.3.0]</span><br><span class="line">at org.apache.spark.SparkContext$$anonfun$38.apply(SparkContext.scala:2232) ~[spark-core_2.11-2.4.0-cdh6.3.0.jar:2.4.0-cdh6.3.0]</span><br><span class="line">at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.11-2.4.0-cdh6.3.0.jar:2.4.0-cdh6.3.0]</span><br><span class="line">at org.apache.spark.scheduler.Task.run(Task.scala:121) ~[spark-core_2.11-2.4.0-cdh6.3.0.jar:2.4.0-cdh6.3.0]</span><br><span class="line">at org.apache.spark.executor.Executor$TaskRunner$$anonfun$11.apply(Executor.scala:407) ~[spark-core_2.11-2.4.0-cdh6.3.0.jar:2.4.0-cdh6.3.0]</span><br><span class="line">at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1408) ~[spark-core_2.11-2.4.0-cdh6.3.0.jar:2.4.0-cdh6.3.0]</span><br><span class="line">at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:413) ~[spark-core_2.11-2.4.0-cdh6.3.0.jar:2.4.0-cdh6.3.0]</span><br><span class="line">... 3 more</span><br></pre></td></tr></table></figure></div><h1><span id="二-异常处理">二、异常处理</span></h1><p>其中<code>org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.doSerialize(LazySimpleSerDe.java:231)</code>函数中有序列化每个字段的逻辑：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Serialize a row of data.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> obj</span></span><br><span class="line"><span class="comment"> *          The row object</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> objInspector</span></span><br><span class="line"><span class="comment"> *          The ObjectInspector for the row object</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> The serialized Writable object</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@see</span> SerDe#serialize(Object, ObjectInspector)</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Writable <span class="title">doSerialize</span><span class="params">(Object obj, ObjectInspector objInspector)</span></span></span><br><span class="line"><span class="function">    <span class="keyword">throws</span> SerDeException </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (objInspector.getCategory() != Category.STRUCT) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> SerDeException(getClass().toString()</span><br><span class="line">        + <span class="string">&quot; can only serialize struct types, but we got: &quot;</span></span><br><span class="line">        + objInspector.getTypeName());</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Prepare the field ObjectInspectors</span></span><br><span class="line">  StructObjectInspector soi = (StructObjectInspector) objInspector;</span><br><span class="line">  List&lt;? extends StructField&gt; fields = soi.getAllStructFieldRefs();</span><br><span class="line">  List&lt;Object&gt; list = soi.getStructFieldsDataAsList(obj);</span><br><span class="line">  List&lt;? extends StructField&gt; declaredFields = (serdeParams.getRowTypeInfo() != <span class="keyword">null</span> &amp;&amp; ((StructTypeInfo) serdeParams.getRowTypeInfo())</span><br><span class="line">      .getAllStructFieldNames().size() &gt; <span class="number">0</span>) ? ((StructObjectInspector) getObjectInspector())</span><br><span class="line">      .getAllStructFieldRefs()</span><br><span class="line">      : <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">  serializeStream.reset();</span><br><span class="line">  serializedSize = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Serialize each field</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; fields.size(); i++) &#123;</span><br><span class="line">    <span class="comment">// Append the separator if needed.</span></span><br><span class="line">    <span class="keyword">if</span> (i &gt; <span class="number">0</span>) &#123;</span><br><span class="line">      serializeStream.write(serdeParams.getSeparators()[<span class="number">0</span>]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// Get the field objectInspector and the field object.</span></span><br><span class="line">    ObjectInspector foi = fields.get(i).getFieldObjectInspector();</span><br><span class="line">    Object f = (list == <span class="keyword">null</span> ? <span class="keyword">null</span> : list.get(i));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (declaredFields != <span class="keyword">null</span> &amp;&amp; i &gt;= declaredFields.size()) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> SerDeException(<span class="string">&quot;Error: expecting &quot;</span> + declaredFields.size()</span><br><span class="line">          + <span class="string">&quot; but asking for field &quot;</span> + i + <span class="string">&quot;\n&quot;</span> + <span class="string">&quot;data=&quot;</span> + obj + <span class="string">&quot;\n&quot;</span></span><br><span class="line">          + <span class="string">&quot;tableType=&quot;</span> + serdeParams.getRowTypeInfo().toString() + <span class="string">&quot;\n&quot;</span></span><br><span class="line">          + <span class="string">&quot;dataType=&quot;</span></span><br><span class="line">          + TypeInfoUtils.getTypeInfoFromObjectInspector(objInspector));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    serializeField(serializeStream, f, foi, serdeParams);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// <span class="doctag">TODO:</span> The copy of data is unnecessary, but there is no work-around</span></span><br><span class="line">  <span class="comment">// since we cannot directly set the private byte[] field inside Text.</span></span><br><span class="line">  serializeCache</span><br><span class="line">      .set(serializeStream.getData(), <span class="number">0</span>, serializeStream.getLength());</span><br><span class="line">  serializedSize = serializeStream.getLength();</span><br><span class="line">  lastOperationSerialize = <span class="keyword">true</span>;</span><br><span class="line">  lastOperationDeserialize = <span class="keyword">false</span>;</span><br><span class="line">  <span class="keyword">return</span> serializeCache;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>for循环中的最后一行<code>serializeField(serializeStream, f, foi, serdeParams);</code>调用的即是异常堆栈中的<code>org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serializeField(LazySimpleSerDe.java:247)</code>函数：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">serializeField</span><span class="params">(ByteStream.Output out, Object obj, ObjectInspector objInspector,</span></span></span><br><span class="line"><span class="function"><span class="params">    LazySerDeParameters serdeParams)</span> <span class="keyword">throws</span> SerDeException </span>&#123;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    serialize(out, obj, objInspector, serdeParams.getSeparators(), <span class="number">1</span>, serdeParams.getNullSequence(),</span><br><span class="line">        serdeParams.isEscaped(), serdeParams.getEscapeChar(), serdeParams.getNeedsEscape());</span><br><span class="line">  &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> SerDeException(e);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>serializeField中调用的serialize函数为异常堆栈中的<code>org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:292)</code>函数：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Serialize the row into the StringBuilder.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> out</span></span><br><span class="line"><span class="comment"> *          The StringBuilder to store the serialized data.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> obj</span></span><br><span class="line"><span class="comment"> *          The object for the current field.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> objInspector</span></span><br><span class="line"><span class="comment"> *          The ObjectInspector for the current Object.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> separators</span></span><br><span class="line"><span class="comment"> *          The separators array.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> level</span></span><br><span class="line"><span class="comment"> *          The current level of separator.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> nullSequence</span></span><br><span class="line"><span class="comment"> *          The byte sequence representing the NULL value.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> escaped</span></span><br><span class="line"><span class="comment"> *          Whether we need to escape the data when writing out</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> escapeChar</span></span><br><span class="line"><span class="comment"> *          Which char to use as the escape char, e.g. &#x27;\\&#x27;</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> needsEscape</span></span><br><span class="line"><span class="comment"> *          Which byte needs to be escaped for 256 bytes. </span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> SerDeException</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">serialize</span><span class="params">(ByteStream.Output out, Object obj,</span></span></span><br><span class="line"><span class="function"><span class="params">    ObjectInspector objInspector, <span class="keyword">byte</span>[] separators, <span class="keyword">int</span> level,</span></span></span><br><span class="line"><span class="function"><span class="params">    Text nullSequence, <span class="keyword">boolean</span> escaped, <span class="keyword">byte</span> escapeChar, <span class="keyword">boolean</span>[] needsEscape)</span></span></span><br><span class="line"><span class="function">    <span class="keyword">throws</span> IOException, SerDeException </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (obj == <span class="keyword">null</span>) &#123;</span><br><span class="line">    out.write(nullSequence.getBytes(), <span class="number">0</span>, nullSequence.getLength());</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">char</span> separator;</span><br><span class="line">  List&lt;?&gt; list;</span><br><span class="line">  <span class="keyword">switch</span> (objInspector.getCategory()) &#123;</span><br><span class="line">  <span class="keyword">case</span> PRIMITIVE:</span><br><span class="line">    LazyUtils.writePrimitiveUTF8(out, obj,</span><br><span class="line">        (PrimitiveObjectInspector) objInspector, escaped, escapeChar,</span><br><span class="line">        needsEscape);</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  <span class="keyword">case</span> LIST:</span><br><span class="line">    ......</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  <span class="keyword">case</span> MAP:</span><br><span class="line">    ......</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  <span class="keyword">case</span> STRUCT:</span><br><span class="line">    ......</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  <span class="keyword">case</span> UNION:</span><br><span class="line">    ......</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  <span class="keyword">default</span>:</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">&quot;Unknown category type: &quot;</span></span><br><span class="line">      + objInspector.getCategory());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>在这个函数中，调用后续针对特定类型的函数对特定类型进行序列化，类型不兼容时则抛出异常。可以看到当前字段数据为空时，有如下逻辑：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (obj == <span class="keyword">null</span>) &#123;</span><br><span class="line">      out.write(nullSequence.getBytes(), <span class="number">0</span>, nullSequence.getLength());</span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></div><p>所以还是可以在<code>LazySimpleSerDe.doSerialize</code>函数中处理每个字段的逻辑中，捕获ClassCastException，并参考serialize函数这种逻辑写入空值，将<code>LazySimpleSerDe.doSerialize</code>中</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">serializeField(serializeStream, f, foi, serdeParams);</span><br></pre></td></tr></table></figure></div><p>改成</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    serializeField(serializeStream, f, foi, serdeParams);</span><br><span class="line">&#125; <span class="keyword">catch</span> (ClassCastException | UnsupportedOperationException e) &#123;</span><br><span class="line">    serializeStream.write(serdeParams.getNullSequence().getBytes(), <span class="number">0</span>, serdeParams.getNullSequence().getLength());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><h1><span id="三-hive-on-spark依赖的hive-jar包部署">三、Hive on Spark依赖的Hive jar包部署</span></h1><p>上面代码修改后，用前一篇文章中的copy_jars.sh脚本将hive*.jar部署后，Hive默认的MR执行引擎已经可以执行本文开始提到的会报错的SQL，但是当Hive使用Spark作为执行引擎时（如beeline中可通过<b>set hive.execution.engine=spark;</b>设置），仍然会报错，猜测Spark使用的Hive依赖包在另外的位置也存放了一份。</p><p>从前面的日志可以看出，一部分日志后面都显示了hive-exec-2.1.1-cdh6.3.0.jar这个jar包名，在部署了CDH的主机上搜索这个jar包：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">shell</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@dev-master2 tmp]# find / -name hive-exec-2.1.1-cdh6.3.0.jar</span><br><span class="line">/opt/cloudera/cm/cloudera-navigator-server/libs/cdh6/hive-exec-2.1.1-cdh6.3.0.jar</span><br><span class="line">/opt/cloudera/cm/cloudera-scm-telepub/libs/cdh6/hive-exec-2.1.1-cdh6.3.0.jar</span><br><span class="line">/opt/cloudera/parcels/CDH-6.3.0-1.cdh6.3.0.p0.1279813/jars/hive-exec-2.1.1-cdh6.3.0.jar</span><br><span class="line">/opt/cloudera/parcels/CDH-6.3.0-1.cdh6.3.0.p0.1279813/lib/spark/hive/hive-exec-2.1.1-cdh6.3.0.jar</span><br><span class="line">/opt/cloudera/parcels/CDH-6.3.0-1.cdh6.3.0.p0.1279813/lib/hive/lib/hive-exec-2.1.1-cdh6.3.0.jar</span><br><span class="line">/opt/cloudera/parcels/CDH-6.3.0-1.cdh6.3.0.p0.1279813/lib/oozie/embedded-oozie-server/webapp/WEB-INF/lib/hive-exec-2.1.1-cdh6.3.0.jar</span><br></pre></td></tr></table></figure></div><p>看起来/opt/cloudera/parcels/CDH-6.3.0-1.cdh6.3.0.p0.1279813/lib/spark/hive/hive-exec-2.1.1-cdh6.3.0.jar这个就是Spark使用的Hive依赖包存放位置，且这个目录下只有一个jar包：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">shell</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@dev-master2 tmp]# ls -lh /opt/cloudera/parcels/CDH-6.3.0-1.cdh6.3.0.p0.1279813/lib/spark/hive/</span><br><span class="line">total 35M</span><br><span class="line">-rw-r--r--. 1 root root 35M Jul 19  2019 hive-exec-2.1.1-cdh6.3.0.jar</span><br></pre></td></tr></table></figure></div><p>所以在copy_jars.sh中添加一句</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp $current_dir&#x2F;hive-exec-2.1.1-cdh6.3.0.jar &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.3.0-1.cdh6.3.0.p0.1279813&#x2F;lib&#x2F;spark&#x2F;hive&#x2F;</span><br></pre></td></tr></table></figure></div><p>再重新部署，经测试，Hive on Spark已经可以查询类型不兼容的类型，结果显示为空值。</p>]]></content>
    
    <summary type="html">
    
      继上一篇之后，又发现了一种新的报错位置。本篇对这种情况进行处理，并验证这种处理方式是否适用于Hive on Spark环境。
    
    </summary>
    
    
    
      <category term="大数据" scheme="http://longwang.live/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Hive" scheme="http://longwang.live/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>一种处理Hive元数据与文件类型不同时SQL查询失败的方法</title>
    <link href="http://longwang.live/2022/03/08/%E4%B8%80%E7%A7%8D%E5%A4%84%E7%90%86Hive%E5%85%83%E6%95%B0%E6%8D%AE%E4%B8%8E%E6%96%87%E4%BB%B6%E7%B1%BB%E5%9E%8B%E4%B8%8D%E5%90%8C%E6%97%B6SQL%E6%9F%A5%E8%AF%A2%E5%A4%B1%E8%B4%A5%E7%9A%84%E6%96%B9%E6%B3%95/"/>
    <id>http://longwang.live/2022/03/08/%E4%B8%80%E7%A7%8D%E5%A4%84%E7%90%86Hive%E5%85%83%E6%95%B0%E6%8D%AE%E4%B8%8E%E6%96%87%E4%BB%B6%E7%B1%BB%E5%9E%8B%E4%B8%8D%E5%90%8C%E6%97%B6SQL%E6%9F%A5%E8%AF%A2%E5%A4%B1%E8%B4%A5%E7%9A%84%E6%96%B9%E6%B3%95/</id>
    <published>2022-03-07T17:29:30.000Z</published>
    <updated>2022-03-26T10:19:45.379Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="一-背景">一、背景</span></h1><p>&ensp;&ensp;&ensp;&ensp;由于Hive的元数据与文件存储分离，且可单独修改表的类型，造成元数据与文件类型不同，这时使用SQL查询数据则会报错。不幸的是，我们就有这种需求，数采的数据同步了一份在Hive中，每天有大量的数据实时写入生成大量小文件；且对列的类型修改等没做限制，列类型可以被改成与之前不兼容的类型，以致于查询报错，通过insert overwrite来合并小文件的任务也一直失败，HDFS上小文件不断增多，严重影响查询效率。<br>&ensp;&ensp;&ensp;&ensp;当前使用的Hive版本为2.1.1-cdh6.3.0（CDH6.3.0），搜索了一下也没有什么配置可以直接让Hive忽略这种类型不同的错误，当前Hive版本也没有高版本似乎有的类型兼容的功能；先简单调试了一下发现Hive Hook功能似乎也拦截不到数据这一步。不得已尝试一下修改源码的方式，却走通了。<br>&ensp;&ensp;&ensp;&ensp;本文记录通过修改Hive（2.1.1-cdh6.3.0）源码的方式，处理Hive元数据与文件类型不同时，SQL查询失败的问题，将类型不兼容的字段查询结果设置为空值。</p><h1><span id="二-分析过程">二、分析过程</span></h1><h2><span id="21-环境及测试数据">2.1 环境及测试数据</span></h2><h3><span id="211-环境">2.1.1 环境</span></h3><p>&ensp;&ensp;&ensp;&ensp;CDH6.3.0，Hive版本为2.1.1-cdh6.3.0，还是调试hiveserver2，调试方法参考之前的《Hive源码调试》文章。顺带一提，github上cloudera/hive已经搜不到了，可能不打算开源了，还好gitee上这位朋友保存了一份<a href="https://gitee.com/gabry/cloudera-hive">https://gitee.com/gabry/cloudera-hive</a> ，有需要的可以自己保存一下这个仓库。</p><h3><span id="212-测试数据">2.1.2 测试数据</span></h3><p>创建一个表t1（我们默认用的parquet格式，本文也只测试过parquet格式数据；分区表也可以，但这里只举一个非分区表例子），插入两条数据；再创建一个列名相同，但id列类型不同的表error_type：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">sql</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> t1(id <span class="type">float</span>,content string) stored <span class="keyword">as</span> parquet;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> t1 vlaues(<span class="number">1.1</span>,<span class="string">&#x27;content1&#x27;</span>),(<span class="number">2.2</span>,<span class="string">&#x27;content2&#x27;</span>);</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> error_type(id <span class="type">int</span>,content string) stored <span class="keyword">as</span> parquet;</span><br></pre></td></tr></table></figure></div><p>在HDFS上直接将t1的数据文件拷到error_type表的目录下：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">shell</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -cp /user/hive/warehouse/testdb.db/t1/000000_0 /user/hive/warehouse/testdb.db/error_type/</span><br></pre></td></tr></table></figure></div><p>这时使用sql查询error_type表则会报错：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2:&#x2F;&#x2F;localhost:10000&gt; select * from error_type;</span><br><span class="line">INFO  : Compiling command(queryId&#x3D;hive_20220306113526_62d5507c-8df1-478b-8f9f-4ea1b8601df9): select * from error_type</span><br><span class="line">INFO  : Semantic Analysis Completed</span><br><span class="line">INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:error_type.id, type:int, comment:null), FieldSchema(name:error_type.content, type:string, comment:null)], properties:null)</span><br><span class="line">INFO  : Completed compiling command(queryId&#x3D;hive_20220306113526_62d5507c-8df1-478b-8f9f-4ea1b8601df9); Time taken: 0.13 seconds</span><br><span class="line">INFO  : Executing command(queryId&#x3D;hive_20220306113526_62d5507c-8df1-478b-8f9f-4ea1b8601df9): select * from error_type</span><br><span class="line">INFO  : Completed executing command(queryId&#x3D;hive_20220306113526_62d5507c-8df1-478b-8f9f-4ea1b8601df9); Time taken: 0.001 seconds</span><br><span class="line">INFO  : OK</span><br><span class="line">Error: java.io.IOException: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassCastException: org.apache.hadoop.io.FloatWritable cannot be cast to org.apache.hadoop.io.IntWritable (state&#x3D;,code&#x3D;0)</span><br></pre></td></tr></table></figure></div><h2><span id="22-select语句异常分析">2.2 select语句异常分析</span></h2><h3><span id="221-异常分析">2.2.1 异常分析</span></h3><p>开始调试，上面的ClassCastException发生处的函数调用栈（从IDEA中复制）为：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">getPrimitiveJavaObject:46, WritableIntObjectInspector (org.apache.hadoop.hive.serde2.objectinspector.primitive)</span><br><span class="line">copyToStandardObject:412, ObjectInspectorUtils (org.apache.hadoop.hive.serde2.objectinspector)</span><br><span class="line">toThriftPayload:170, SerDeUtils (org.apache.hadoop.hive.serde2)</span><br><span class="line">convert:49, ThriftFormatter (org.apache.hadoop.hive.serde2.thrift)</span><br><span class="line">process:94, ListSinkOperator (org.apache.hadoop.hive.ql.exec)</span><br><span class="line">forward:882, Operator (org.apache.hadoop.hive.ql.exec)</span><br><span class="line">process:95, SelectOperator (org.apache.hadoop.hive.ql.exec)</span><br><span class="line">forward:882, Operator (org.apache.hadoop.hive.ql.exec)</span><br><span class="line">process:130, TableScanOperator (org.apache.hadoop.hive.ql.exec)</span><br><span class="line">pushRow:438, FetchOperator (org.apache.hadoop.hive.ql.exec)</span><br><span class="line">pushRow:430, FetchOperator (org.apache.hadoop.hive.ql.exec)</span><br><span class="line">fetch:146, FetchTask (org.apache.hadoop.hive.ql.exec)</span><br><span class="line">getResults:2227, Driver (org.apache.hadoop.hive.ql)</span><br><span class="line">getNextRowSet:491, SQLOperation (org.apache.hive.service.cli.operation)</span><br><span class="line">getOperationNextRowSet:297, OperationManager (org.apache.hive.service.cli.operation)</span><br><span class="line">fetchResults:869, HiveSessionImpl (org.apache.hive.service.cli.session)</span><br><span class="line">invoke:-1, GeneratedMethodAccessor5 (sun.reflect)</span><br><span class="line">invoke:43, DelegatingMethodAccessorImpl (sun.reflect)</span><br><span class="line">invoke:498, Method (java.lang.reflect)</span><br><span class="line">invoke:78, HiveSessionProxy (org.apache.hive.service.cli.session)</span><br><span class="line">access$000:36, HiveSessionProxy (org.apache.hive.service.cli.session)</span><br><span class="line">run:63, HiveSessionProxy$1 (org.apache.hive.service.cli.session)</span><br><span class="line">doPrivileged:-1, AccessController (java.security)</span><br><span class="line">doAs:422, Subject (javax.security.auth)</span><br><span class="line">doAs:1962, UserGroupInformation (org.apache.hadoop.security)</span><br><span class="line">invoke:59, HiveSessionProxy (org.apache.hive.service.cli.session)</span><br><span class="line">fetchResults:-1, $Proxy39 (com.sun.proxy)</span><br><span class="line">fetchResults:507, CLIService (org.apache.hive.service.cli)</span><br><span class="line">FetchResults:708, ThriftCLIService (org.apache.hive.service.cli.thrift)</span><br><span class="line">getResult:1717, TCLIService$Processor$FetchResults (org.apache.hive.service.rpc.thrift)</span><br><span class="line">getResult:1702, TCLIService$Processor$FetchResults (org.apache.hive.service.rpc.thrift)</span><br><span class="line">process:39, ProcessFunction (org.apache.thrift)</span><br><span class="line">process:39, TBaseProcessor (org.apache.thrift)</span><br><span class="line">process:56, TSetIpAddressProcessor (org.apache.hive.service.auth)</span><br><span class="line">run:286, TThreadPoolServer$WorkerProcess (org.apache.thrift.server)</span><br><span class="line">runWorker:1149, ThreadPoolExecutor (java.util.concurrent)</span><br><span class="line">run:624, ThreadPoolExecutor$Worker (java.util.concurrent)</span><br><span class="line">run:748, Thread (java.lang)</span><br></pre></td></tr></table></figure></div><p>WritableIntObjectInspector.getPrimitiveJavaObject:46这个方法为：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Object <span class="title">getPrimitiveJavaObject</span><span class="params">(Object o)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> o == <span class="keyword">null</span> ? <span class="keyword">null</span> : Integer.valueOf(((IntWritable) o).get());</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div><p>此时参数为：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">this &#x3D; &#123;WritableIntObjectInspector@10276&#125; </span><br><span class="line"> typeInfo &#x3D; &#123;PrimitiveTypeInfo@10277&#125; &quot;int&quot;</span><br><span class="line">o &#x3D; &#123;FloatWritable@10262&#125; &quot;1.1&quot;</span><br></pre></td></tr></table></figure></div><p>这里将从文件中读取的FloatWritable类型的对象，转换为根据表元数据int类型对应的IntWritable类型，出现ClassCastException。</p><p>对日志中看到的HiveException类的构造函数下断点，可知道抛出HiveException的位置为函数调用栈process:94, ListSinkOperator (org.apache.hadoop.hive.ql.exec)这一行对应的这个函数：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line">  <span class="meta">@SuppressWarnings(&quot;unchecked&quot;)</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(Object row, <span class="keyword">int</span> tag)</span> <span class="keyword">throws</span> HiveException </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      res.add(fetcher.convert(row, inputObjInspectors[<span class="number">0</span>]));</span><br><span class="line">      numRows++;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> HiveException(e);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div><h3><span id="222-捕获异常位置">2.2.2 捕获异常位置</span></h3><p>异常抛出后，被捕获并抛出HiveException之前的几个栈中函数：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">getPrimitiveJavaObject:46, WritableIntObjectInspector (org.apache.hadoop.hive.serde2.objectinspector.primitive)</span><br><span class="line">copyToStandardObject:412, ObjectInspectorUtils (org.apache.hadoop.hive.serde2.objectinspector)</span><br><span class="line">toThriftPayload:170, SerDeUtils (org.apache.hadoop.hive.serde2)</span><br><span class="line">convert:49, ThriftFormatter (org.apache.hadoop.hive.serde2.thrift)</span><br></pre></td></tr></table></figure></div><p><code>getPrimitiveJavaObject:46, WritableIntObjectInspector</code>显然是特定类型的实现，不适合在这里捕获异常；<code>copyToStandardObject:412, ObjectInspectorUtils</code>函数本身逻辑比较复杂；<code>toThriftPayload:170, SerDeUtils</code>和<code>convert:49, ThriftFormatter</code>都可以，<code>convert:49, ThriftFormatter</code>刚好有个循环处理一行数据的每个字段，在这里处理看起来比较清晰，</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Object <span class="title">convert</span><span class="params">(Object row, ObjectInspector rowOI)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">  StructObjectInspector structOI = (StructObjectInspector) rowOI;</span><br><span class="line">  List&lt;? extends StructField&gt; fields = structOI.getAllStructFieldRefs();</span><br><span class="line">  Object[] converted = <span class="keyword">new</span> Object[fields.size()];</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span> ; i &lt; converted.length; i++) &#123;</span><br><span class="line">    StructField fieldRef = fields.get(i);</span><br><span class="line">    Object field = structOI.getStructFieldData(row, fieldRef);</span><br><span class="line">    converted[i] = field == <span class="keyword">null</span> ? <span class="keyword">null</span> :</span><br><span class="line">        SerDeUtils.toThriftPayload(field, fieldRef.getFieldObjectInspector(), protocol);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> converted;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>将生成converted[i]的那行改为：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">  converted[i] = field == <span class="keyword">null</span> ? <span class="keyword">null</span> :</span><br><span class="line">          SerDeUtils.toThriftPayload(field, fieldRef.getFieldObjectInspector(), protocol);</span><br><span class="line">&#125; <span class="keyword">catch</span> (ClassCastException e) &#123;</span><br><span class="line">    converted[i] = <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>这样修改后（部署见后面章节）执行select * from error_type不会抛异常了，查询的2条数据id字段都为null。</p><h2><span id="23-insert-overwrite语句异常分析">2.3 insert overwrite语句异常分析</span></h2><h3><span id="231-异常分析">2.3.1 异常分析</span></h3><p>&ensp;&ensp;&ensp;&ensp;本以为就这样修改一下就可以了，尝试执行合并小文件的SQL：<code>insert overwrite table error_type select * from error_type</code>还会报错，日志里打印的函数调用栈如下：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Caused by: java.lang.ClassCastException: org.apache.hadoop.io.FloatWritable cannot be cast to org.apache.hadoop.io.IntWritable</span><br><span class="line">at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableIntObjectInspector.get(WritableIntObjectInspector.java:36)</span><br><span class="line">at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter$IntDataWriter.write(DataWritableWriter.java:385)</span><br><span class="line">at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter$GroupDataWriter.write(DataWritableWriter.java:199)</span><br><span class="line">at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter$MessageDataWriter.write(DataWritableWriter.java:215)</span><br><span class="line">at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter.write(DataWritableWriter.java:88)</span><br><span class="line">at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriteSupport.write(DataWritableWriteSupport.java:60)</span><br><span class="line">at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriteSupport.write(DataWritableWriteSupport.java:32)</span><br><span class="line">at org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:123)</span><br><span class="line">at org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:179)</span><br><span class="line">at org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:46)</span><br><span class="line">at org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper.write(ParquetRecordWriterWrapper.java:136)</span><br><span class="line">at org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper.write(ParquetRecordWriterWrapper.java:149)</span><br><span class="line">at org.apache.hadoop.hive.ql.exec.FileSinkOperator.process(FileSinkOperator.java:769)</span><br><span class="line">at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:882)</span><br><span class="line">at org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:95)</span><br><span class="line">at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:882)</span><br><span class="line">at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:130)</span><br><span class="line">at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:146)</span><br><span class="line">at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:484)</span><br></pre></td></tr></table></figure></div><h3><span id="232-捕获异常位置">2.3.2 捕获异常位置</span></h3><p>经过亿点调试分析（这些SQL有MR任务，任务会提交到Yarn，先设置参数<code>set hive.exec.mode.local.auto=true;</code>让Hive以本地模式运行该SQL，否则断点不会触发），接近抛异常位置（是否可以作为规律）的这个方法org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter.GroupDataWriter#write也有与前面<code>ThriftFormatter.convert:49</code>方法类似的通过循环写每一个字段的功能：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(Object value)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; structFields.size(); i++) &#123;</span><br><span class="line">    StructField field = structFields.get(i);</span><br><span class="line">    Object fieldValue = inspector.getStructFieldData(value, field);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (fieldValue != <span class="keyword">null</span>) &#123;</span><br><span class="line">      String fieldName = field.getFieldName();</span><br><span class="line">      DataWriter writer = structWriters[i];</span><br><span class="line"></span><br><span class="line">      recordConsumer.startField(fieldName, i);</span><br><span class="line">      writer.write(fieldValue);</span><br><span class="line">      recordConsumer.endField(fieldName, i);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>其中<code>writer.write(fieldValue)</code>就是异常信息打印的调用栈中的<code>org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter$GroupDataWriter.write(DataWritableWriter.java:199)</code>位置。</p><p>这里有一行<code>Object fieldValue = inspector.getStructFieldData(value, field);</code>，经过调试可以发现，这行代码和前面捕获异常的方法<code>convert:49, ThriftFormatter</code>中的<code>Object field = structOI.getStructFieldData(row, fieldRef);</code>调用的都是<code>org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector#getStructFieldData</code>：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="meta">@SuppressWarnings(&quot;unchecked&quot;)</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Object <span class="title">getStructFieldData</span><span class="params">(Object data, StructField fieldRef)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (data == <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// We support both List&lt;Object&gt; and Object[]</span></span><br><span class="line">  <span class="comment">// so we have to do differently.</span></span><br><span class="line">  <span class="keyword">boolean</span> isArray = data.getClass().isArray();</span><br><span class="line">  <span class="keyword">if</span> (!isArray &amp;&amp; !(data <span class="keyword">instanceof</span> List)) &#123;</span><br><span class="line">    <span class="keyword">if</span> (!warned) &#123;</span><br><span class="line">      LOG.warn(<span class="string">&quot;Invalid type for struct &quot;</span> + data.getClass());</span><br><span class="line">      LOG.warn(<span class="string">&quot;ignoring similar errors.&quot;</span>);</span><br><span class="line">      warned = <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> data;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">int</span> listSize = (isArray ? ((Object[]) data).length : ((List&lt;Object&gt;) data)</span><br><span class="line">      .size());</span><br><span class="line">  MyField f = (MyField) fieldRef;</span><br><span class="line">  <span class="keyword">if</span> (fields.size() != listSize &amp;&amp; !warned) &#123;</span><br><span class="line">    <span class="comment">// <span class="doctag">TODO:</span> remove this</span></span><br><span class="line">    warned = <span class="keyword">true</span>;</span><br><span class="line">    LOG.warn(<span class="string">&quot;Trying to access &quot;</span> + fields.size()</span><br><span class="line">        + <span class="string">&quot; fields inside a list of &quot;</span> + listSize + <span class="string">&quot; elements: &quot;</span></span><br><span class="line">        + (isArray ? Arrays.asList((Object[]) data) : (List&lt;Object&gt;) data));</span><br><span class="line">    LOG.warn(<span class="string">&quot;ignoring similar errors.&quot;</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">int</span> fieldID = f.getFieldID();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (fieldID &gt;= listSize) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (isArray) &#123;</span><br><span class="line">    <span class="keyword">return</span> ((Object[]) data)[fieldID];</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> ((List&lt;Object&gt;) data).get(fieldID);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p><code>StandardStructObjectInspector#getStructFieldData</code>方法一个参数为从文件从读取的一行数据，第二个参数为<code>org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.MyField</code>的实例，其中有要取的字段的下标，这个方法大概功能就是根据下标从一行数据中取数，但是没做类型判断。<code>MyField</code>中也有与表的元数据中字段类型对应的ObjectInspector，可以使用ObjectInspector来读取一下本次获取的字段数据，如果类型冲突则捕获ClassCastException，并让本方法返回空值，后续的读写流程本字段都是null，这样无论对于之前的select语句还是insert overwrite语句，都可以达到本文想要的效果。</p><p>由于我们定义的Hive表都是用的原始类型，所以调用objectInspector实现的<code>org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector</code>接口中的方法<code>getPrimitiveJavaObject</code>，通过多态来实现各种类型数据的读取，<code>StandardStructObjectInspector#getStructFieldData</code>函数最后一个if else部分改为：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">Object objValue;</span><br><span class="line"><span class="keyword">if</span> (fieldID &gt;= listSize) &#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> (isArray) &#123;</span><br><span class="line">  objValue = ((Object[]) data)[fieldID];</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  objValue = ((List&lt;Object&gt;) data).get(fieldID);</span><br><span class="line">&#125;</span><br><span class="line">ObjectInspector objectInspector = f.getFieldObjectInspector();</span><br><span class="line"><span class="keyword">if</span> (Category.PRIMITIVE.equals(objectInspector.getCategory())) &#123;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    ((PrimitiveObjectInspector) objectInspector).getPrimitiveJavaObject(objValue);</span><br><span class="line">  &#125; <span class="keyword">catch</span> (ClassCastException | UnsupportedOperationException e) &#123;</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    UnsupportedOperationException:</span></span><br><span class="line"><span class="comment">      如Hive列类型为String，这里获取到的objectInspector为ParquetStringInspector的实例，</span></span><br><span class="line"><span class="comment">      但org.apache.hadoop.hive.ql.io.parquet.serde.primitive.ParquetStringInspector.getPrimitiveJavaObject中，</span></span><br><span class="line"><span class="comment">      参数不是那个方法中做了判断的那几种类型时就会抛UnsupportedOperationException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    objValue = <span class="keyword">null</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> objValue;</span><br></pre></td></tr></table></figure></div><p>（修改了这部分后，对于原始类型，其实前面2.2.2节中的捕获异常可以删除）</p><h3><span id="232-读数据readrow异常">2.3.2 读数据（readRow）异常</span></h3><p>在有的表执行insert overwrite时，遇到了下面两个错误（其实是同一种）：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Caused by: java.lang.UnsupportedOperationException: Cannot inspect org.apache.hadoop.io.LongWritable</span><br><span class="line">at org.apache.hadoop.hive.ql.io.parquet.serde.primitive.ParquetStringInspector.getPrimitiveJavaObject(ParquetStringInspector.java:77)</span><br><span class="line">at org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.getLong(PrimitiveObjectInspectorUtils.java:709)</span><br><span class="line">at org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter$LongConverter.convert(PrimitiveObjectInspectorConverter.java:182)</span><br><span class="line">at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters$StructConverter.convert(ObjectInspectorConverters.java:416)</span><br><span class="line">at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.readRow(MapOperator.java:126)</span><br><span class="line">at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.access$200(MapOperator.java:89)</span><br><span class="line">at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:483)</span><br></pre></td></tr></table></figure></div><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Caused by: java.lang.ClassCastException: org.apache.hadoop.io.FloatWritable cannot be cast to org.apache.hadoop.io.IntWritable</span><br><span class="line">at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableIntObjectInspector.get(WritableIntObjectInspector.java:36)</span><br><span class="line">at org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.getDouble(PrimitiveObjectInspectorUtils.java:755)</span><br><span class="line">at org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.getFloat(PrimitiveObjectInspectorUtils.java:796)</span><br><span class="line">at org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter$FloatConverter.convert(PrimitiveObjectInspectorConverter.java:211)</span><br><span class="line">at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters$StructConverter.convert(ObjectInspectorConverters.java:416)</span><br><span class="line">at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.readRow(MapOperator.java:126)</span><br><span class="line">at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.access$200(MapOperator.java:89)</span><br><span class="line">at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:483)</span><br></pre></td></tr></table></figure></div><p>都是在MapOperator 483行调用MapOperator$MapOpCtx.readRow中的错误（2.3.1节中日志显示MapOperator 484调用MapOperator$MapOpCtx.forward中抛出的异常），只是两个日志中最后出错的数据类型不同。没分析代码，还是用与之前类似的方法，在<code>org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.StructConverter#convert</code>方法中也有个循环处理每个字段的功能，将异常日志中<code>ObjectInspectorConverters.java:416</code>指出的这一行代码：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">outputFieldValue = fieldConverters.get(f).convert(inputFieldValue);</span><br></pre></td></tr></table></figure></div><p>改成：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">  outputFieldValue = fieldConverters.get(f).convert(inputFieldValue);</span><br><span class="line">&#125; <span class="keyword">catch</span> (ClassCastException | UnsupportedOperationException e) &#123;</span><br><span class="line">  outputFieldValue = <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p><font color="red"><b>修改到这一步后，我们遇到的那些元数据与与文件不兼容类型的表都能正常查询了，也可以通过insert overwrite合并小文件了。</b></font></p><h1><span id="三-代码示例及结论">三、代码示例及结论</span></h1><p>修改的代码都提交到了fork的这个仓库中，可查看这个提交记录 <a href="https://gitee.com/Ox3E6/cloudera-hive/commit/1e31127162b3bb29716580692c2d1fe30543f057">https://gitee.com/Ox3E6/cloudera-hive/commit/1e31127162b3bb29716580692c2d1fe30543f057</a></p><p>目前还不熟悉Hive代码迷宫中的细节和一些整体流程，仅仅根据报错位置尝试添加一些处理功能，如2.3.1节和2.3.3节都是在实际数据处理过程中抛出的异常。还是需要多做测试，遇到一个问题处理一个，只要转换成null后Hive的读写及其他逻辑不报错就行。</p><h1><span id="四-cdh集群中部署修改后的jar包">四、CDH集群中部署修改后的jar包</span></h1><p>&ensp;&ensp;&ensp;&ensp;修改的代码都位于hive-serde模块中，但是由于其他hive模块也引入了hive-serde依赖（没意识到这点之前，只替换了hive-serde jar包，每次还是抛出HiveException异常，重新编译调试了几次，甚至以为出现了灵异事件或是又要发现什么至今未知的Java异常捕获优先级技巧…），所以最简单的方法就是把编译打包后<font color="red"><b>lib目录下hive开头的jar包</b></font>全部拷过去。</p><p>CDH机器上，搜索hive jar包位置以hive-serde jar包为例：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@dev-master2 ~]# find &#x2F; -name hive-serde-2.1.1-cdh6.3.0.jar</span><br><span class="line"></span><br><span class="line">&#x2F;opt&#x2F;cloudera&#x2F;cm&#x2F;cloudera-navigator-server&#x2F;libs&#x2F;cdh6&#x2F;hive-serde-2.1.1-cdh6.3.0.jar</span><br><span class="line">&#x2F;opt&#x2F;cloudera&#x2F;cm&#x2F;cloudera-scm-telepub&#x2F;libs&#x2F;cdh6&#x2F;hive-serde-2.1.1-cdh6.3.0.jar</span><br><span class="line">&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.3.0-1.cdh6.3.0.p0.1279813&#x2F;jars&#x2F;hive-serde-2.1.1-cdh6.3.0.jar</span><br><span class="line">&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.3.0-1.cdh6.3.0.p0.1279813&#x2F;lib&#x2F;hive&#x2F;lib&#x2F;hive-serde-2.1.1-cdh6.3.0.jar</span><br><span class="line">&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.3.0-1.cdh6.3.0.p0.1279813&#x2F;lib&#x2F;oozie&#x2F;embedded-oozie-server&#x2F;webapp&#x2F;WEB-INF&#x2F;lib&#x2F;hive-serde-2.1.1-cdh6.3.0.jar</span><br></pre></td></tr></table></figure></div><p>有的是符号链接，只需要放入这3个目录：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;opt&#x2F;cloudera&#x2F;cm&#x2F;common_jars&#x2F;</span><br><span class="line">&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.3.0-1.cdh6.3.0.p0.1279813&#x2F;jars&#x2F;</span><br><span class="line">&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.3.0-1.cdh6.3.0.p0.1279813&#x2F;lib&#x2F;oozie&#x2F;embedded-oozie-server&#x2F;webapp&#x2F;WEB-INF&#x2F;lib&#x2F;</span><br></pre></td></tr></table></figure></div><p>后两个目录下jar包文件名直接就是打包出来的jar包名，但是common_jars目录下的jar包文件名中有一串不知道什么算法生成的hash，形如：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@dev-master2 scripts]# ls -lh &#x2F;opt&#x2F;cloudera&#x2F;cm&#x2F;common_jars&#x2F;hive* </span><br><span class="line">-rw-r--r--. 1 root root  46K Jul 19  2019 &#x2F;opt&#x2F;cloudera&#x2F;cm&#x2F;common_jars&#x2F;hive-ant-2.1.1-cdh6.3.0.f857dabb5222c1969c9f4087c8bfaac3.jar</span><br><span class="line">-rw-r--r--. 1 root root  12K Jul 19  2019 &#x2F;opt&#x2F;cloudera&#x2F;cm&#x2F;common_jars&#x2F;hive-classification-2.1.1-cdh6.3.0.c2ac9c5cf1fbb22aeda542f287ecbaa4.jar</span><br><span class="line">-rw-r--r--. 1 root root  46K Jul 19  2019 &#x2F;opt&#x2F;cloudera&#x2F;cm&#x2F;common_jars&#x2F;hive-cli-2.1.1-cdh6.3.0.f8741782bcbf8b4b58f537da6346e0ff.jar</span><br><span class="line">-rw-r--r--. 1 root root 324K Jul 19  2019 &#x2F;opt&#x2F;cloudera&#x2F;cm&#x2F;common_jars&#x2F;hive-common-1.1.0-cdh5.12.0.10beb989e3d6a390afce045b1e865bde.jar</span><br><span class="line">-rw-r--r--. 1 root root 429K Jul 19  2019 &#x2F;opt&#x2F;cloudera&#x2F;cm&#x2F;common_jars&#x2F;hive-common-2.1.1-cdh6.3.0.87dadce3138dc2c5c2e696cc6f6f7927.jar</span><br><span class="line">......</span><br></pre></td></tr></table></figure></div><p>以前替换yarn一个有并发修改问题的jar包也遇到这种情况，但是jar包替换后使用原来一样的带hash的文件名，也没有报校验失败的错误。</p><p>所以可将以下脚本与所有打包后lib目录下的hive*.jar放在一个目录下，将目录拷到CDH上每一台（通过一些脚本）机器上，并在每一台（通过一些脚本）机器上运行该脚本，替换hive的jar包，然后<font color="red"><b>重启Hive</b></font>即可。</p><p>copy_jars.sh（带hash的那部分可从机器上拷出来，再通过正则替换生成）</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">shell</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/usr/bin/env bash</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 参考/user/bin/hive脚本：Reference: http://stackoverflow.com/questions/59895/can-a-bash-script-tell-what-directory-its-stored-in</span></span><br><span class="line">current_dir=$( cd -- &quot;$( dirname -- &quot;$&#123;BASH_SOURCE[0]&#125;&quot; )&quot; &amp;&gt; /dev/null &amp;&amp; pwd )</span><br><span class="line">echo &quot;current_dir: $current_dir&quot;</span><br><span class="line">cp $current_dir/hive*.jar /opt/cloudera/parcels/CDH-6.3.0-1.cdh6.3.0.p0.1279813/jars/</span><br><span class="line">cp $current_dir/hive*.jar /opt/cloudera/parcels/CDH-6.3.0-1.cdh6.3.0.p0.1279813/lib/oozie/embedded-oozie-server/webapp/WEB-INF/lib/</span><br><span class="line"></span><br><span class="line">cp $current_dir/hive-ant-2.1.1-cdh6.3.0.jar /opt/cloudera/cm/common_jars/hive-ant-2.1.1-cdh6.3.0.f857dabb5222c1969c9f4087c8bfaac3.jar</span><br><span class="line">cp $current_dir/hive-classification-2.1.1-cdh6.3.0.jar /opt/cloudera/cm/common_jars/hive-classification-2.1.1-cdh6.3.0.c2ac9c5cf1fbb22aeda542f287ecbaa4.jar</span><br><span class="line">cp $current_dir/hive-cli-2.1.1-cdh6.3.0.jar /opt/cloudera/cm/common_jars/hive-cli-2.1.1-cdh6.3.0.f8741782bcbf8b4b58f537da6346e0ff.jar</span><br><span class="line">cp $current_dir/hive-common-1.1.0-cdh5.12.0.jar /opt/cloudera/cm/common_jars/hive-common-1.1.0-cdh5.12.0.10beb989e3d6a390afce045b1e865bde.jar</span><br><span class="line">cp $current_dir/hive-common-2.1.1-cdh6.3.0.jar /opt/cloudera/cm/common_jars/hive-common-2.1.1-cdh6.3.0.87dadce3138dc2c5c2e696cc6f6f7927.jar</span><br><span class="line">cp $current_dir/hive-exec-2.1.1-cdh6.3.0.jar /opt/cloudera/cm/common_jars/hive-exec-2.1.1-cdh6.3.0.15d37ff81bca70d35b904a6946abea49.jar</span><br><span class="line">cp $current_dir/hive-jdbc-2.1.1-cdh6.3.0.jar /opt/cloudera/cm/common_jars/hive-jdbc-2.1.1-cdh6.3.0.a9016068a26246ac47c4b2637db33adb.jar</span><br><span class="line">cp $current_dir/hive-llap-client-2.1.1-cdh6.3.0.jar /opt/cloudera/cm/common_jars/hive-llap-client-2.1.1-cdh6.3.0.701f1dfc66958f0d8feab78d602b9cb6.jar</span><br><span class="line">cp $current_dir/hive-llap-common-2.1.1-cdh6.3.0.jar /opt/cloudera/cm/common_jars/hive-llap-common-2.1.1-cdh6.3.0.6c733dcdfa1e52ce79dc1b0066220a00.jar</span><br><span class="line">cp $current_dir/hive-llap-server-2.1.1-cdh6.3.0.jar /opt/cloudera/cm/common_jars/hive-llap-server-2.1.1-cdh6.3.0.105d9633082dfb213b9d390dc3df8087.jar</span><br><span class="line">cp $current_dir/hive-llap-tez-2.1.1-cdh6.3.0.jar /opt/cloudera/cm/common_jars/hive-llap-tez-2.1.1-cdh6.3.0.47ac2463acf7de1929b57c4da5ac7f41.jar</span><br><span class="line">cp $current_dir/hive-metastore-1.1.0-cdh5.12.0.jar /opt/cloudera/cm/common_jars/hive-metastore-1.1.0-cdh5.12.0.f439e1b26177542bfc57e428717a265a.jar</span><br><span class="line">cp $current_dir/hive-metastore-2.1.1-cdh6.3.0.jar /opt/cloudera/cm/common_jars/hive-metastore-2.1.1-cdh6.3.0.4a407e44f9168f014f41edd4a56d5028.jar</span><br><span class="line">cp $current_dir/hive-orc-2.1.1-cdh6.3.0.jar /opt/cloudera/cm/common_jars/hive-orc-2.1.1-cdh6.3.0.0d1f0cf02d1bdad572cca211654c64af.jar</span><br><span class="line">cp $current_dir/hive-serde-1.1.0-cdh5.12.0.jar /opt/cloudera/cm/common_jars/hive-serde-1.1.0-cdh5.12.0.62c4570f4681c0698b9f5f5ab6baab4a.jar</span><br><span class="line">cp $current_dir/hive-serde-2.1.1-cdh6.3.0.jar /opt/cloudera/cm/common_jars/hive-serde-2.1.1-cdh6.3.0.bde9116deea651dbf085034565504351.jar</span><br><span class="line">cp $current_dir/hive-service-2.1.1-cdh6.3.0.jar /opt/cloudera/cm/common_jars/hive-service-2.1.1-cdh6.3.0.0c28a52a856414cb45d0b827bd7884e9.jar</span><br><span class="line">cp $current_dir/hive-service-rpc-2.1.1-cdh6.3.0.jar /opt/cloudera/cm/common_jars/hive-service-rpc-2.1.1-cdh6.3.0.fde12a48a558128e4d15bfb47f90cfb4.jar</span><br><span class="line">cp $current_dir/hive-shims-1.1.0-cdh5.12.0.jar /opt/cloudera/cm/common_jars/hive-shims-1.1.0-cdh5.12.0.2698b9ffda7580409fc299d986f41ded.jar</span><br><span class="line">cp $current_dir/hive-shims-2.1.1-cdh6.3.0.jar /opt/cloudera/cm/common_jars/hive-shims-2.1.1-cdh6.3.0.a151f9e3d14dfeb5bb2b34e0b2ef8a28.jar</span><br><span class="line">cp $current_dir/hive-storage-api-2.1.1-cdh6.3.0.jar /opt/cloudera/cm/common_jars/hive-storage-api-2.1.1-cdh6.3.0.fb98d759511d27287bcd20e48b40f961.jar</span><br></pre></td></tr></table></figure></div><h1><span id="五-可能的其他方案">五、可能的其他方案？</span></h1><ul><li>如从Hive表的INPUTFORMAT切入</li><li>更加熟悉Hive流程后，看其他地方是否能更方便地处理或全局处理</li><li>有空看看Hive3的兼容怎么做的，“学习学习”</li></ul><p><a href="/img/bg-images/nengyongjiuxing.jpg" data-fancybox="group" data-caption="能用就行" class="fancybox"><img alt="能用就行" title="能用就行" data-src="/img/bg-images/nengyongjiuxing.jpg" class="lazyload"></a></p>]]></content>
    
    <summary type="html">
    
      通过修改Hive（2.1.1-cdh6.3.0）源码的方式，处理Hive元数据与文件类型不同时，SQL查询失败的问题，将类型不兼容的字段查询结果设置为空值。
    
    </summary>
    
    
    
      <category term="大数据" scheme="http://longwang.live/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Hive" scheme="http://longwang.live/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive源码调试</title>
    <link href="http://longwang.live/2021/03/23/Hive%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95/"/>
    <id>http://longwang.live/2021/03/23/Hive%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95/</id>
    <published>2021-03-23T13:04:14.000Z</published>
    <updated>2021-03-23T13:23:23.070Z</updated>
    
    <content type="html"><![CDATA[<p>[toc]</p><h2><span id="1-概述">1、概述</span></h2><p>&emsp;&emsp;最近用到了Hive Hook的一些功能，每次打包后都要将jar包拷贝到CDH集群中，还要重启Hive，很麻烦。且传入Hook类中的HookContext对象用Json工具类转换成Json时某些情况下有问题，想看其中的具体内容很不方便。尝试用alibaba的arthas工具attach到hiveserver2上通过monitor方法查看参数，发现该工具无法打印层次太深的成员变量。So，研究了一下Hive源码调试方法，这样在调试Hive Hook这类应用比较方便，如果后续要看Hive内部原理之类的，也方便通过调试学习。</p><p>&emsp;&emsp;本文介绍的方法通过搭建一个单节点hadoop，然后编译Hive源码，通过hive启动相关hive服务时，在要调试的hive服务的启动命令中添加–debug选项，通过远程调试的方式来调试Hive的相关服务。（不知道网上有些地方说的不需要hadoop能否运行哈，反正我没试成功）</p><p>&emsp;&emsp;使用的Hive版本为CDH6.3.0对应的hive版本。</p><h2><span id="2-安装hadoop">2、安装Hadoop</span></h2><p>&emsp;&emsp;简单配置一个可用的单节点hadoop即可。</p><h3><span id="21-下载及配置">2.1 下载及配置</span></h3><ul><li>下载<a href="https://archive.apache.org/dist/hadoop/common/hadoop-3.0.0/hadoop-3.0.0.tar.gz" target="_blank" rel="noopener">hdaoop3.0.0</a>，解压并修改配置文件：</li><li>hadoop-env.sh添加JAVA_HOME<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME&#x3D;&#x2F;usr&#x2F;lib&#x2F;jvm&#x2F;java-8-openjdk-amd64</span><br></pre></td></tr></table></figure></div></li><li>core-site.xml<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">　　　　&lt;name&gt;fs.default.name&lt;&#x2F;name&gt;</span><br><span class="line">　　　　&lt;value&gt;hdfs:&#x2F;&#x2F;kali:9000&lt;&#x2F;value&gt;</span><br><span class="line">　　&lt;&#x2F;property&gt;</span><br><span class="line">　　&lt;property&gt;</span><br><span class="line">　　　　&lt;name&gt;hadoop.tmp.dir&lt;&#x2F;name&gt;</span><br><span class="line">　　　　&lt;value&gt;file:&#x2F;&#x2F;&#x2F;root&#x2F;tool&#x2F;BD&#x2F;hadoop.data.dir&#x2F;tmp&lt;&#x2F;value&gt;</span><br><span class="line">　　&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hadoop.proxyuser.sqoop2.hosts&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;*&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hadoop.proxyuser.sqoop2.groups&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;*&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hadoop.proxyuser.root.hosts&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;*&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hadoop.proxyuser.root.groups&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;*&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;fs.file.impl&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;org.apache.hadoop.fs.LocalFileSystem&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;fs.hdfs.impl&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;org.apache.hadoop.hdfs.DistributedFileSystem&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure></div></li><li>hdfs-site.xml<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">　　　　&lt;name&gt;dfs.replication&lt;&#x2F;name&gt;</span><br><span class="line">　　　　&lt;value&gt;1&lt;&#x2F;value&gt;</span><br><span class="line">　　&lt;&#x2F;property&gt;</span><br><span class="line">　　&lt;property&gt;</span><br><span class="line">　　　　&lt;name&gt;dfs.namenode.name.dir&lt;&#x2F;name&gt;</span><br><span class="line">　　　　&lt;value&gt;file:&#x2F;&#x2F;&#x2F;root&#x2F;tool&#x2F;BD&#x2F;hadoop.data.dir&#x2F;dfs&#x2F;name&lt;&#x2F;value&gt;</span><br><span class="line">　　&lt;&#x2F;property&gt;</span><br><span class="line">　　&lt;property&gt;</span><br><span class="line">　　　　&lt;name&gt;dfs.datanode.data.dir&lt;&#x2F;name&gt;</span><br><span class="line">　　　　&lt;value&gt;file:&#x2F;&#x2F;&#x2F;root&#x2F;tool&#x2F;BD&#x2F;hadoop.data.dir&#x2F;dfs&#x2F;data&lt;&#x2F;value&gt;</span><br><span class="line">　　&lt;&#x2F;property&gt;</span><br><span class="line">　　&lt;property&gt;</span><br><span class="line">　　　　&lt;name&gt;dfs.namenode.secondary.http-address&lt;&#x2F;name&gt;</span><br><span class="line">　　　　&lt;value&gt;kali:9001&lt;&#x2F;value&gt;</span><br><span class="line">　　&lt;&#x2F;property&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure></div></li><li>mapred-site.xml<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">　　　　&lt;name&gt;mapreduce.framework.name&lt;&#x2F;name&gt;</span><br><span class="line">　　　　&lt;value&gt;yarn&lt;&#x2F;value&gt;</span><br><span class="line">　　&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.jobhistory.address&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;kali:10020&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.map.memory.mb&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;1024&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.reduce.memory.mb&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;2048&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.app.mapreduce.am.env&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;HADOOP_MAPRED_HOME&#x3D;&#x2F;root&#x2F;tool&#x2F;BD&#x2F;hadoop-3.0.0&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.map.env&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;HADOOP_MAPRED_HOME&#x3D;&#x2F;root&#x2F;tool&#x2F;BD&#x2F;hadoop-3.0.0&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.reduce.env&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;HADOOP_MAPRED_HOME&#x3D;&#x2F;root&#x2F;tool&#x2F;BD&#x2F;hadoop-3.0.0&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure></div></li><li>yarn-site.xml<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- Site specific YARN configuration properties --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.acl.enable&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;0&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.resourcemanager.hostname&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;kali&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.nodemanager.aux-services&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;mapreduce_shuffle&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.log-aggregation-enable&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;true&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.log-aggregation.retain-seconds&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;604800&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;false&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">     &lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.app.mapreduce.am.resource.mb&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;1024&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;1024&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;4096&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;16384&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure></div><h3><span id="22-配置环境变量">2.2 配置环境变量</span></h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># 启动hive时提示需要配置</span><br><span class="line">export HADOOP_HOME&#x3D;&#x2F;root&#x2F;tool&#x2F;BD&#x2F;hadoop-3.0.0</span><br><span class="line"># 用root用户启动hadoop提示需要配置用户</span><br><span class="line">export HDFS_NAMENODE_USER&#x3D;root</span><br><span class="line">export HDFS_DATANODE_USER&#x3D;root</span><br><span class="line">export HDFS_SECONDARYNAMENODE_USER&#x3D;root</span><br><span class="line">export YARN_RESOURCEMANAGER_USER&#x3D;root</span><br><span class="line">export YARN_NODEMANAGER_USER&#x3D;root</span><br><span class="line">export HADOOP_MAPRED_HOME&#x3D;$HADOOP_HOME</span><br><span class="line">export HADOOP_YARN_HOME&#x3D;$HADOOP_HOME</span><br><span class="line"></span><br><span class="line">export PATH&#x3D;$PATH:$HADOOP_HOME&#x2F;sbin:$HADOOP_HOME&#x2F;bin</span><br></pre></td></tr></table></figure></div><h3><span id="23-格式化namenode">2.3 格式化namenode</span></h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$HADOOP_HOME&#x2F;sbin&#x2F;hdfs namenode -format</span><br></pre></td></tr></table></figure></div><h3><span id="24-启动hdoop">2.4 启动hdoop</span></h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$HADOOP_HOME&#x2F;sbin&#x2F;start-all.sh</span><br></pre></td></tr></table></figure></div>&emsp;&emsp;确保yarn的ResourceManager、NodeManager，hdfs的NameNode、SecondaryNameNode、DataNode服务启动即可。<h2><span id="3-编译hive">3、编译Hive</span></h2><h3><span id="31-编译">3.1 编译</span></h3>&emsp;&emsp;克隆<a href="https://github.com/cloudera/hive" target="_blank" rel="noopener">cloudera/hive</a>仓库，切换到cdh6.3.0分支（当前使用的版本），可以设置一下项目根目录下的pom.xml中的仓库地址,换成比较快的仓库，然后在项目根目录下执行<code>mvn clean package -DskipTests -Pdist</code>，构建可发布的tar包。运行完后生成的文件在<code>packagine [hive-package]</code>模块中，如下为相关文件列表：<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">drwxrwxrwx 1 root root         0 Mar 17 08:58 apache-hive-2.1.1-cdh6.3.0-bin</span><br><span class="line">-rwxrwxrwx 1 root root 207735011 Mar 17 08:58 apache-hive-2.1.1-cdh6.3.0-bin.tar.gz</span><br><span class="line">-rwxrwxrwx 1 root root  25238408 Mar 17 08:59 apache-hive-2.1.1-cdh6.3.0-jdbc.jar</span><br><span class="line">-rwxrwxrwx 1 root root  19744989 Mar 17 08:59 apache-hive-2.1.1-cdh6.3.0-src.tar.gz</span><br></pre></td></tr></table></figure></div><h3><span id="32-配置hive">3.2 配置hive</span></h3>&emsp;&emsp;打包后，<code>packaging/target/apache-hive-2.1.1-cdh6.3.0-bin/apache-hive-2.1.1-cdh6.3.0-bin</code>目录下是未压缩的相关hive文件，后续操作可以都在这个目录下执行，<b>所以后面以该目录为当前目录</b>。</li></ul><p>&emsp;&emsp;拷贝一个配置文件，<code>cp conf/hive-default.xml.template conf/hive-site.xml</code>，在<code>conf/hive-site.xml</code>中修改或添加一些必要的配置：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;system:java.io.tmpdir&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;&#x2F;tmp&#x2F;hive&#x2F;java&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;system:user.name&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;$&#123;user.name&#125;&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;!-- 元数据库配置 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">  &lt;name&gt;javax.jdo.option.ConnectionURL&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;jdbc:mysql:&#x2F;&#x2F;localhost:3306&#x2F;hive_debug?createDatabaseIfNotExist&#x3D;true&amp;characterEncoding&#x3D;utf8&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">  &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;root&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">  &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;goodluck&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">  &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;com.mysql.jdbc.Driver&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure></div><h3><span id="33-初始化元数据库">3.3 初始化元数据库</span></h3><p>&emsp;&emsp;(tips:可以提前将<code>scripts/metastore/upgrade/mysql/hive-schema-2.1.0.mysql.sql</code>中的建表语句编码改成utf8，可以避免后续为了解决中文注释、表名、列名再来修改，不过有几个表改成utf8编码会报主键太长的错误，还是要保持latin1编码，可以在改成utf8后运行初始化语句，遇到报错的表时，将那个表的编码再改回latin1编码再执行初始化语句)</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin&#x2F;schematool -dbType mysql -initSchema</span><br></pre></td></tr></table></figure></div><h2><span id="4-调试">4、调试</span></h2><p>&emsp;&emsp;目前已经可以开始调试hive源码，采用远程调试的方式。</p><ol><li>将编写并打包好的HiveHook项目的jar包拷到lib目录下，再在conf/hive-site.xml中配置hook类：<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hive.exec.post.hooks&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;tech.xxx.HiveHook&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;description&gt;</span><br><span class="line">    Comma-separated list of post-execution hooks to be invoked for each statement. </span><br><span class="line">    A post-execution hook is specified as the name of a Java class which implements the </span><br><span class="line">    org.apache.hadoop.hive.ql.hooks.ExecuteWithHookContext interface.</span><br><span class="line">  &lt;&#x2F;description&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure></div></li><li>先启动hive metastore：<code>bin/hive --service metastore</code>;</li><li>由于是要调试hive hook，属于hiveserver2的功能，所以以调试模式启动hiveserver2服务：<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># bin&#x2F;hive --debug --service hiveserver2</span><br><span class="line">Listening for transport dt_socket at address: 8000</span><br></pre></td></tr></table></figure></div></li><li>idea打开cloudera hive项目（之前已经切换到cdh6.3.0分支），在idea项目结构中的Libraries中，添加自己编写的HiveHook项目的Jar包，这样调试时idea才能看到自己的代码；再添加一个Remote Run/Debug Configurations，Host填写为<code>localhost</code>，Port填写为<code>8000</code>，然后点击调试，则idea连接上hivesever2，hiveserver2会继续启动并运行。再打上断点，实践后发现调用post hook的位置在<code>org.apache.hadoop.hive.ql.Driver</code>类1960行的<code>((ExecuteWithHookContext) peh).run(hookContext);</code>处，可以在这里打个断点（当然可以在其他想打的位置打断点，比如先配置一个Hive已实现的Hook类，在run方法打断点）；</li><li>启动beeline客户端，开始执行SQL并调试：<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin&#x2F;beeline -u jdbc:hive2:&#x2F;&#x2F;localhost:10000 -n root</span><br></pre></td></tr></table></figure></div>&emsp;&emsp;然后执行sql，断点则会触发，即可以在idea中调试hive源码，若想调试metastore或beeline，也可在启动这些功能时加上<code>--debug</code>，仍然使用远程的方式进行调试。</li></ol><h2><span id="参考链接">参考链接</span></h2><ul><li><a href="https://cwiki.apache.org/confluence/display/Hive/DeveloperDocs" target="_blank" rel="noopener">Hive Developer Documentation</a></li><li><a href="https://blog.csdn.net/u013289115/article/details/112464043" target="_blank" rel="noopener">Hive源码本地IDEA调试的正确姿势</a></li><li><a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html" target="_blank" rel="noopener">Hadoop: Setting up a Single Node Cluster</a></li></ul>]]></content>
    
    <summary type="html">
    
      一种Hive源码（cloudera/hive cdh6.3.0）调试方法
    
    </summary>
    
    
    
      <category term="大数据" scheme="http://longwang.live/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Hive" scheme="http://longwang.live/tags/Hive/"/>
    
      <category term="Hadoop" scheme="http://longwang.live/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>一种动态更新flink任务配置的方法</title>
    <link href="http://longwang.live/2020/04/25/%E4%B8%80%E7%A7%8D%E5%8A%A8%E6%80%81%E6%9B%B4%E6%96%B0flink%E4%BB%BB%E5%8A%A1%E9%85%8D%E7%BD%AE%E7%9A%84%E6%96%B9%E6%B3%95/"/>
    <id>http://longwang.live/2020/04/25/%E4%B8%80%E7%A7%8D%E5%8A%A8%E6%80%81%E6%9B%B4%E6%96%B0flink%E4%BB%BB%E5%8A%A1%E9%85%8D%E7%BD%AE%E7%9A%84%E6%96%B9%E6%B3%95/</id>
    <published>2020-04-25T07:17:00.000Z</published>
    <updated>2020-04-25T08:23:54.278Z</updated>
    
    <content type="html"><![CDATA[<p>[toc]</p><h2><span id="1-原理">1  原理</span></h2><p>参考<a href="https://www.cnblogs.com/littleCode/p/10362717.html" target="_blank" rel="noopener">Flink/Spark 如何实现动态更新作业配置</a>，讲得比较详细，这篇的文章的参考参考文献也可阅读一下。flink任务配置动态更新的实现方法，可通过添加一个控制流，将数据流与控制流连接后，再读取控制流中的消息来更新数据流处理逻辑中的参数，这样即实现了数据流处理逻辑中配置参数动态更新的方法，无需重启任务或修改代码。</p><h2><span id="2-例整数过滤">2 例，整数过滤</span></h2><p>本例中，控制流发送的整数作为除数，如果数据流中的整数除以控制流中的整数余数为0，则将数据流中的数据向下游发送。</p><h3><span id="21-并行度为1">2.1  并行度为1</span></h3><p>先设置并行为1，测试效果。定义2个数据源，一个dataStream产生数据流，其中数据为0到19，一个configStream产生控制流，向流中发送一个整数2。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.setParallelism(<span class="number">1</span>);</span><br><span class="line">DataStream&lt;Integer&gt; configStream = env.addSource(<span class="keyword">new</span> SourceFunction&lt;Integer&gt;() &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = -<span class="number">4529394264795596001L</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">boolean</span> running = <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;Integer&gt; sourceContext)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        sourceContext.collect(<span class="number">2</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        running = <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line">DataStream&lt;Integer&gt; dataStream = env.addSource(<span class="keyword">new</span> SourceFunction&lt;Integer&gt;() &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = -<span class="number">1885959149409672550L</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">boolean</span> running = <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;Integer&gt; sourceContext)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; running &amp;&amp; i &lt; <span class="number">20</span>; i += <span class="number">1</span>) &#123;</span><br><span class="line">            sourceContext.collect(i);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        running = <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure></div><p>连接数据流与控制流</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ConnectedStreams connectedStreams = dataStream.connect(configStream);</span><br></pre></td></tr></table></figure></div><p>处理连接后的流，如下代码所示，先定义除数为1，processElement2函数处理的控制流发来的数据，接收到后更新除数的值；processElement1处理的数据流发来的数据，如果数据除以除数的余数为0，则通过collector发出去。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;Integer&gt; resultStream = connectedStreams.process(<span class="keyword">new</span> CoProcessFunction&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">8870659158532269705L</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Integer divisor = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement1</span><span class="params">(Integer o, Context context, Collector&lt;Integer&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        log.info(<span class="string">"[Data] Received &#123;&#125;, divisor &#123;&#125;"</span>, o, divisor);</span><br><span class="line">        <span class="keyword">if</span> (o % divisor == <span class="number">0</span>) &#123;</span><br><span class="line">            collector.collect(o);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement2</span><span class="params">(Integer o, Context context, Collector&lt;Integer&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        log.info(<span class="string">"[Divisor] Received &#123;&#125;"</span>, o);</span><br><span class="line">        divisor = o;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line">resultStream.print();</span><br><span class="line">env.execute();</span><br></pre></td></tr></table></figure></div><p>主要输出如下：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (1&#x2F;1)] INFO test.flink.streaming.connectstream.Test2  - [Data] Received 0, divisor 1</span><br><span class="line">0</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (1&#x2F;1)] INFO test.flink.streaming.connectstream.Test2  - [Divisor] Received 2</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (1&#x2F;1)] INFO test.flink.streaming.connectstream.Test2  - [Data] Received 1, divisor 2</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (1&#x2F;1)] INFO test.flink.streaming.connectstream.Test2  - [Data] Received 2, divisor 2</span><br><span class="line">2</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (1&#x2F;1)] INFO test.flink.streaming.connectstream.Test2  - [Data] Received 3, divisor 2</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (1&#x2F;1)] INFO test.flink.streaming.connectstream.Test2  - [Data] Received 4, divisor 2</span><br><span class="line">4</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (1&#x2F;1)] INFO test.flink.streaming.connectstream.Test2  - [Data] Received 5, divisor 2</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (1&#x2F;1)] INFO test.flink.streaming.connectstream.Test2  - [Data] Received 6, divisor 2</span><br><span class="line">6</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (1&#x2F;1)] INFO test.flink.streaming.connectstream.Test2  - [Data] Received 7, divisor 2</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (1&#x2F;1)] INFO test.flink.streaming.connectstream.Test2  - [Data] Received 8, divisor 2</span><br><span class="line">8</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (1&#x2F;1)] INFO test.flink.streaming.connectstream.Test2  - [Data] Received 9, divisor 2</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (1&#x2F;1)] INFO test.flink.streaming.connectstream.Test2  - [Data] Received 10, divisor 2</span><br><span class="line">10</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (1&#x2F;1)] INFO test.flink.streaming.connectstream.Test2  - [Data] Received 11, divisor 2</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (1&#x2F;1)] INFO test.flink.streaming.connectstream.Test2  - [Data] Received 12, divisor 2</span><br><span class="line">12</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (1&#x2F;1)] INFO test.flink.streaming.connectstream.Test2  - [Data] Received 13, divisor 2</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (1&#x2F;1)] INFO test.flink.streaming.connectstream.Test2  - [Data] Received 14, divisor 2</span><br><span class="line">14</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (1&#x2F;1)] INFO test.flink.streaming.connectstream.Test2  - [Data] Received 15, divisor 2</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (1&#x2F;1)] INFO test.flink.streaming.connectstream.Test2  - [Data] Received 16, divisor 2</span><br><span class="line">16</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (1&#x2F;1)] INFO test.flink.streaming.connectstream.Test2  - [Data] Received 17, divisor 2</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (1&#x2F;1)] INFO test.flink.streaming.connectstream.Test2  - [Data] Received 18, divisor 2</span><br><span class="line">18</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (1&#x2F;1)] INFO test.flink.streaming.connectstream.Test2  - [Data] Received 19, divisor 2</span><br></pre></td></tr></table></figure></div><p>可见，数据流中的0到达时，除数仍为1,0被发出并打印出来。然后接收到控制流的2，除数被更新为2，后面数据流的数据只有偶数被打印出来。这已经实现了除数由1变为2的动态更新。</p><h3><span id="22-并行度大于1">2.2  并行度大于1</span></h3><p>注释<code>env.setParallelism(1);</code>，flink默认设置的并行度与处理器核数相同，在上面<code>CoProcessFunction</code>匿名类的2个处理数据方法中打印相关信息的同时打印出线程id。输出结果如下：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"> [Co-Process -&gt; Sink: Print to Std. Out (2&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Data] Thead 81 received 4, divisor 1</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (4&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Data] Thead 83 received 6, divisor 1</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (8&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Data] Thead 87 received 10, divisor 1</span><br><span class="line">4&gt; 6</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (10&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Data] Thead 89 received 0, divisor 1</span><br><span class="line">8&gt; 10</span><br><span class="line">2&gt; 4</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (12&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Data] Thead 91 received 2, divisor 1</span><br><span class="line">10&gt; 0</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (7&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Data] Thead 86 received 9, divisor 1</span><br><span class="line">7&gt; 9</span><br><span class="line">12&gt; 2</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (3&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Data] Thead 82 received 5, divisor 1</span><br><span class="line">3&gt; 5</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (1&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Data] Thead 76 received 3, divisor 1</span><br><span class="line">1&gt; 3</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (6&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Data] Thead 85 received 8, divisor 1</span><br><span class="line">6&gt; 8</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (9&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Data] Thead 88 received 11, divisor 1</span><br><span class="line">9&gt; 11</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (5&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Data] Thead 84 received 7, divisor 1</span><br><span class="line">5&gt; 7</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (11&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Data] Thead 90 received 1, divisor 1</span><br><span class="line">11&gt; 1</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (5&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Data] Thead 84 received 19, divisor 1</span><br><span class="line">5&gt; 19</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (10&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Data] Thead 89 received 12, divisor 1</span><br><span class="line">10&gt; 12</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (3&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Data] Thead 82 received 17, divisor 1</span><br><span class="line">3&gt; 17</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (4&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Data] Thead 83 received 18, divisor 1</span><br><span class="line">4&gt; 18</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (12&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Data] Thead 91 received 14, divisor 1</span><br><span class="line">12&gt; 14</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (11&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Data] Thead 90 received 13, divisor 1</span><br><span class="line">11&gt; 13</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (1&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Divisor] Thread 76, received 2</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (2&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Data] Thead 81 received 16, divisor 1</span><br><span class="line">2&gt; 16</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (1&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Data] Thead 76 received 15, divisor 2</span><br></pre></td></tr></table></figure></div><p>只有id为76的线程收到了控制流中的2，并过滤了数据流中的数据15，而其他线程中的除数一直为1，没有过滤数据。</p><p>因此，应将控制流的数据进行广播，将控制流数据源定义代码修改为如下内容：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"> DataStream&lt;Integer&gt; configStream = env.addSource(<span class="keyword">new</span> SourceFunction&lt;Integer&gt;() &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = -<span class="number">4529394264795596001L</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">boolean</span> running = <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;Integer&gt; sourceContext)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        sourceContext.collect(<span class="number">2</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        running = <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;).broadcast();</span><br></pre></td></tr></table></figure></div><p>再次运行后打印的信息如下：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (9&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Data] Thead 88 received 0, divisor 1</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (2&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Divisor] Thread 77, received 2</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (4&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Divisor] Thread 79, received 2</span><br><span class="line">9&gt; 0</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (7&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Data] Thead 86 received 10, divisor 1</span><br><span class="line">7&gt; 10</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (1&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Data] Thead 76 received 4, divisor 1</span><br><span class="line">1&gt; 4</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (7&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Divisor] Thread 86, received 2</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (12&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Data] Thead 91 received 3, divisor 1</span><br><span class="line">12&gt; 3</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (1&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Divisor] Thread 76, received 2</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (5&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Data] Thead 84 received 8, divisor 1</span><br><span class="line">5&gt; 8</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (1&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Data] Thead 76 received 16, divisor 2</span><br><span class="line">1&gt; 16</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (12&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Divisor] Thread 91, received 2</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (9&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Divisor] Thread 88, received 2</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (12&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Data] Thead 91 received 15, divisor 2</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (2&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Data] Thead 77 received 5, divisor 2</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (4&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Data] Thead 79 received 7, divisor 2</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (6&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Data] Thead 85 received 9, divisor 1</span><br><span class="line">6&gt; 9</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (11&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Data] Thead 90 received 2, divisor 1</span><br><span class="line">11&gt; 2</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (10&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Data] Thead 89 received 1, divisor 1</span><br><span class="line">10&gt; 1</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (8&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Data] Thead 87 received 11, divisor 1</span><br><span class="line">8&gt; 11</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (11&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Divisor] Thread 90, received 2</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (10&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Divisor] Thread 89, received 2</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (3&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Divisor] Thread 78, received 2</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (10&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Data] Thead 89 received 13, divisor 2</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (8&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Divisor] Thread 87, received 2</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (11&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Data] Thead 90 received 14, divisor 2</span><br><span class="line">11&gt; 14</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (2&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Data] Thead 77 received 17, divisor 2</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (6&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Divisor] Thread 85, received 2</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (9&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Data] Thead 88 received 12, divisor 2</span><br><span class="line">9&gt; 12</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (5&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Divisor] Thread 84, received 2</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (4&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Data] Thead 79 received 19, divisor 2</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (3&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Data] Thead 78 received 6, divisor 2</span><br><span class="line">3&gt; 6</span><br><span class="line">[Co-Process -&gt; Sink: Print to Std. Out (3&#x2F;12)] INFO test.flink.streaming.connectstream.Test2  - [Data] Thead 78 received 18, divisor 2</span><br><span class="line">3&gt; 18</span><br></pre></td></tr></table></figure></div><p>可见，每个线程都收到了控制流的数据2，如，id为91的线程，收到数据3时除数还是1,3被发出并打印了，接收到控制流中的除数2后，再接收到数据7时，就将奇数7过滤了。</p><h3><span id="23-完整代码">2.3  完整代码</span></h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> test.flink.streaming.connectstream;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> lombok.extern.slf4j.Slf4j;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.ConnectedStreams;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.co.CoProcessFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.source.SourceFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"><span class="keyword">import</span> org.apache.log4j.ConsoleAppender;</span><br><span class="line"><span class="keyword">import</span> org.apache.log4j.Level;</span><br><span class="line"><span class="keyword">import</span> org.apache.log4j.Logger;</span><br><span class="line"><span class="keyword">import</span> org.apache.log4j.PatternLayout;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * ClassName： Test2</span></span><br><span class="line"><span class="comment"> * Description：</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> 0x3E6</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@version</span> 1.0.0</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span> 2020/4/25 9:57</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Slf</span>4j</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Test2</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Logger root = Logger.getRootLogger();</span><br><span class="line">        root.setLevel(Level.INFO);</span><br><span class="line">        root.addAppender(<span class="keyword">new</span> ConsoleAppender(<span class="keyword">new</span> PatternLayout(<span class="string">"[%t] %p %c %x - %m%n"</span>)));</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"><span class="comment">//        env.setParallelism(1);</span></span><br><span class="line">        log.info(<span class="string">"Parallelism &#123;&#125;"</span>, env.getParallelism());</span><br><span class="line">        DataStream&lt;Integer&gt; configStream = env.addSource(<span class="keyword">new</span> SourceFunction&lt;Integer&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = -<span class="number">4529394264795596001L</span>;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">boolean</span> running = <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;Integer&gt; sourceContext)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                sourceContext.collect(<span class="number">2</span>);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                running = <span class="keyword">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).broadcast();</span><br><span class="line">        DataStream&lt;Integer&gt; dataStream = env.addSource(<span class="keyword">new</span> SourceFunction&lt;Integer&gt;() &#123;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = -<span class="number">1885959149409672550L</span>;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">boolean</span> running = <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;Integer&gt; sourceContext)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; running &amp;&amp; i &lt; <span class="number">20</span>; i += <span class="number">1</span>) &#123;</span><br><span class="line">                    sourceContext.collect(i);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                running = <span class="keyword">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        ConnectedStreams connectedStreams = dataStream.connect(configStream);</span><br><span class="line">        DataStream&lt;Integer&gt; resultStream = connectedStreams.process(<span class="keyword">new</span> CoProcessFunction&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">8870659158532269705L</span>;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">private</span> Integer divisor = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement1</span><span class="params">(Integer o, Context context, Collector&lt;Integer&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                log.info(<span class="string">"[Data] Thead &#123;&#125; received &#123;&#125;, divisor &#123;&#125;"</span>, Thread.currentThread().getId(), o, divisor);</span><br><span class="line">                <span class="keyword">if</span> (o % divisor == <span class="number">0</span>) &#123;</span><br><span class="line">                    collector.collect(o);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement2</span><span class="params">(Integer o, Context context, Collector&lt;Integer&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                log.info(<span class="string">"[Divisor] Thread &#123;&#125;, received &#123;&#125;"</span>, Thread.currentThread().getId(), o);</span><br><span class="line">                divisor = o;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        resultStream.print();</span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><h2><span id="参考链接">参考链接</span></h2><ul><li><a href="https://www.cnblogs.com/littleCode/p/10362717.html" target="_blank" rel="noopener">Flink/Spark 如何实现动态更新作业配置</a></li><li><a href="http://www.54tianzhisheng.cn/2020/02/22/flink-nacos/" target="_blank" rel="noopener">Flink 整合 Nacos，让 Flink 作业配置动态更新不再是难事</a></li><li><a href="http://www.54tianzhisheng.cn/2020/02/23/flink-apollo/" target="_blank" rel="noopener">Flink 整合 Apollo，动态更新 Flink 作业配置</a></li></ul>]]></content>
    
    <summary type="html">
    
      通过添加控制流的方式实现Flink任务配置的动态更新
    
    </summary>
    
    
    
      <category term="大数据" scheme="http://longwang.live/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flink" scheme="http://longwang.live/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>一种处理Sqoop导出过程中数据的方法</title>
    <link href="http://longwang.live/2020/02/21/%E4%B8%80%E7%A7%8D%E5%A4%84%E7%90%86Sqoop%E5%AF%BC%E5%87%BA%E8%BF%87%E7%A8%8B%E4%B8%AD%E6%95%B0%E6%8D%AE%E7%9A%84%E6%96%B9%E6%B3%95/"/>
    <id>http://longwang.live/2020/02/21/%E4%B8%80%E7%A7%8D%E5%A4%84%E7%90%86Sqoop%E5%AF%BC%E5%87%BA%E8%BF%87%E7%A8%8B%E4%B8%AD%E6%95%B0%E6%8D%AE%E7%9A%84%E6%96%B9%E6%B3%95/</id>
    <published>2020-02-21T09:14:11.000Z</published>
    <updated>2020-02-21T10:22:10.793Z</updated>
    
    <content type="html"><![CDATA[<h2><span id="一-java代码调用sqoop-api导出数据">一、Java代码调用Sqoop API导出数据</span></h2><p>当前测试用大数据集群版本:cdh6.3.2，Sqoop依赖包的版本为1.4.7-cdh6.3.2。调用Sqoop API的Java代码如下：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> blog;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> lombok.extern.slf4j.Slf4j;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.log4j.BasicConfigurator;</span><br><span class="line"><span class="keyword">import</span> org.apache.log4j.Level;</span><br><span class="line"><span class="keyword">import</span> org.apache.log4j.Logger;</span><br><span class="line"><span class="keyword">import</span> org.apache.sqoop.Sqoop;</span><br><span class="line"><span class="keyword">import</span> org.apache.sqoop.mapreduce.hcat.SqoopHCatUtilities;</span><br><span class="line"><span class="keyword">import</span> org.apache.sqoop.tool.SqoopTool;</span><br><span class="line"><span class="keyword">import</span> org.apache.sqoop.util.OptionsFileUtil;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> 0x3E6</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span> 2020/02/05 20:58 PM</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Slf</span>4j</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> String[] getArgs() &#123;</span><br><span class="line">        String tb = <span class="string">"tb1"</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> String[]&#123;</span><br><span class="line">                <span class="string">"--connect"</span>, <span class="string">"jdbc:mysql://192.168.0.101:3306/test?useSSL=false"</span>,</span><br><span class="line">                <span class="string">"--username"</span>, <span class="string">"test"</span>,</span><br><span class="line">                <span class="string">"--password"</span>, <span class="string">"G00d!1uck"</span>,</span><br><span class="line">                <span class="string">"--table"</span>, tb,</span><br><span class="line">                <span class="string">"--hcatalog-database"</span>, <span class="string">"test"</span>,</span><br><span class="line">                <span class="string">"--hcatalog-table"</span>, tb</span><br><span class="line">        &#125;;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">execSqoop</span><span class="params">(String toolName, String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        String[] expandArguments = OptionsFileUtil.expandArguments(args);</span><br><span class="line">        SqoopTool tool = SqoopTool.getTool(toolName);</span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        <span class="comment">// 以本地模式运行MapReduce程序，这样可以调试MapReduce的整个过程，连Mapper中的map方法都能调试</span></span><br><span class="line">        conf.set(<span class="string">"mapreduce.framework.name"</span>, <span class="string">"local"</span>);</span><br><span class="line">        conf.set(<span class="string">"custom.checkColumn"</span>, <span class="string">"id"</span>);</span><br><span class="line">        conf.set(<span class="string">"custom.lastValue"</span>, <span class="string">"2"</span>);</span><br><span class="line">        Configuration loadPlugins = SqoopTool.loadPlugins(conf);</span><br><span class="line">        Sqoop sqoop = <span class="keyword">new</span> Sqoop(tool, loadPlugins);</span><br><span class="line">        <span class="keyword">return</span> Sqoop.runSqoop(sqoop, expandArguments);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">exportData</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        log.info(<span class="string">"&#123;&#125;"</span>, execSqoop(<span class="string">"export"</span>, getArgs()));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Logger.getRootLogger().setLevel(Level.INFO);</span><br><span class="line">        BasicConfigurator.configure();</span><br><span class="line">        System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"hdfs"</span>);</span><br><span class="line">        exportData();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><h2><span id="二-部分导出过程分析">二、部分导出过程分析</span></h2><p>ExportTool中</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">private void exportTable(SqoopOptions options, String tableName) &#123;</span><br><span class="line">    ...</span><br><span class="line">    &#x2F;&#x2F; INSERT-based export.</span><br><span class="line">    &#x2F;&#x2F; 调用MySQLManager.exportTable</span><br><span class="line">      manager.exportTable(context);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>而MySQLManager的继承关系如下：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MySQLManager-&gt;InformationSchemaManager-&gt;CatalogQueryManager-&gt;GenericJdbcManager-&gt;SqlManager</span><br></pre></td></tr></table></figure></div><p>且只有SqlManager实现了exportTable方法，所以实际调用的SqlManager的exportTable方法:</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;**</span><br><span class="line"> * Export data stored in HDFS into a table in a database.</span><br><span class="line"> *&#x2F;</span><br><span class="line">public void exportTable(org.apache.sqoop.manager.ExportJobContext context)</span><br><span class="line">    throws IOException, ExportException &#123;</span><br><span class="line">  context.setConnManager(this);</span><br><span class="line">  JdbcExportJob exportJob &#x3D; new JdbcExportJob(context, getParquetJobConfigurator().createParquetExportJobConfigurator());</span><br><span class="line">  exportJob.runExport();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>在exportJob.runExport方法中执行了配置MapReduce相关操作，由于JdbcExportJob继承了ExportJobBase且未重写runExport方法，所以实际调用的ExportJobBase中的runExport方法：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">public void runExport() throws ExportException, IOException &#123;</span><br><span class="line"></span><br><span class="line">    ......</span><br><span class="line"></span><br><span class="line">    Job job &#x3D; createJob(conf);</span><br><span class="line">    try &#123;</span><br><span class="line">      &#x2F;&#x2F; Set the external jar to use for the job.</span><br><span class="line">      job.getConfiguration().set(&quot;mapred.jar&quot;, ormJarFile);</span><br><span class="line">      if (options.getMapreduceJobName() !&#x3D; null) &#123;</span><br><span class="line">        job.setJobName(options.getMapreduceJobName());</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      propagateOptionsToJob(job);</span><br><span class="line">      if (isHCatJob) &#123;</span><br><span class="line">        LOG.info(&quot;Configuring HCatalog for export job&quot;);</span><br><span class="line">        SqoopHCatUtilities hCatUtils &#x3D; SqoopHCatUtilities.instance();</span><br><span class="line">        hCatUtils.configureHCat(options, job, cmgr, tableName,</span><br><span class="line">          job.getConfiguration());</span><br><span class="line">      &#125;</span><br><span class="line">      &#x2F;&#x2F; 使用hcatalog导出数据时，配置的InputFormat类为org.apache.sqoop.mapreduce.hcat.SqoopHCatExportFormat</span><br><span class="line">      configureInputFormat(job, tableName, tableClassName, null);</span><br><span class="line">      &#x2F;&#x2F; 从ExportJobBase的getOutputFormatClass方法获取，若不是batch模式则使用的OutPutFormat类为org.apache.sqoop.mapreduce.ExportOutputFormat</span><br><span class="line">      configureOutputFormat(job, tableName, tableClassName);</span><br><span class="line">      &#x2F;&#x2F; 当前this对象为JdbcExportJob，调用的ExportJobBase的configureMapper，</span><br><span class="line">      &#x2F;&#x2F; 其中调用JdbcExportJob中的getMapperClass方法，该方法中判断用了hcatalog则直接返回</span><br><span class="line">      &#x2F;&#x2F; SqoopHCatUtilities.getExportMapperClass()即org.apache.sqoop.mapreduce.hcat.SqoopHCatExportMapper</span><br><span class="line">      configureMapper(job, tableName, tableClassName);</span><br><span class="line">      configureNumTasks(job);</span><br><span class="line">      cacheJars(job, context.getConnManager());</span><br><span class="line"></span><br><span class="line">      jobSetup(job);</span><br><span class="line">      setJob(job);</span><br><span class="line">      boolean success &#x3D; runJob(job);</span><br><span class="line">      if (!success) &#123;</span><br><span class="line">        LOG.error(&quot;Export job failed!&quot;);</span><br><span class="line">        throw new ExportException(&quot;Export job failed!&quot;);</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      if (options.isValidationEnabled()) &#123;</span><br><span class="line">        validateExport(tableName, conf, job);</span><br><span class="line">      &#125;</span><br><span class="line">        ......</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><h2><span id="三-一种处理sqoop导出过程中数据的方法">三、一种处理Sqoop导出过程中数据的方法</span></h2><p>如上面runExport方法中注释所述，导出时配置的Mapper通过<code>SqoopHCatUtilities.getExportMapperClass()</code>获取，实际获取的是该工具类中的static变量<code>exportMapperClass</code>，该成员变量在<code>SqoopHCatUtilities</code>的static代码块中赋值为类<code>org.apache.sqoop.mapreduce.hcat.SqoopHCatExportMapper</code>，且整个导出过程都未修改，<code>SqoopHCatExportMapper</code>内容参考<a href="https://github.com/cloudera/sqoop/blob/cdh6.3.2/src/java/org/apache/sqoop/mapreduce/hcat/SqoopHCatExportMapper.java" target="_blank" rel="noopener">SqoopHCatExportMapper.java</a>。</p><p>因此，可在运行Sqoop命令之前，修改<code>SqoopHCatUtilities</code>的<code>exportMapperClass</code>的值，添加自定义逻辑，对导出的数据进行处理或过滤。</p><p>如，有一个打印导出过程中所有数据的需求（当然实际上肯定没有这么无聊的需求），将<code>SqoopHCatExportMapper</code>类拷出来，修改为如下内容：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> blog;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.commons.logging.Log;</span><br><span class="line"><span class="keyword">import</span> org.apache.commons.logging.LogFactory;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hive.hcatalog.data.HCatRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.sqoop.lib.SqoopRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.sqoop.mapreduce.AutoProgressMapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.sqoop.mapreduce.hcat.SqoopHCatExportHelper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * A mapper that works on combined hcat splits.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ModifiedSqoopHCatExportMapper</span></span></span><br><span class="line"><span class="class">        <span class="keyword">extends</span></span></span><br><span class="line"><span class="class">        <span class="title">AutoProgressMapper</span>&lt;<span class="title">WritableComparable</span>, <span class="title">HCatRecord</span>,</span></span><br><span class="line"><span class="class">                <span class="title">SqoopRecord</span>, <span class="title">WritableComparable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> Log LOG = LogFactory</span><br><span class="line">            .getLog(ModifiedSqoopHCatExportMapper<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">    <span class="keyword">private</span> SqoopHCatExportHelper helper;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span></span></span><br><span class="line"><span class="function">            <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.setup(context);</span><br><span class="line"></span><br><span class="line">        Configuration conf = context.getConfiguration();</span><br><span class="line">        helper = <span class="keyword">new</span> SqoopHCatExportHelper(conf);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(WritableComparable key, HCatRecord value,</span></span></span><br><span class="line"><span class="function"><span class="params">                    Context context)</span></span></span><br><span class="line"><span class="function">            <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// value的class为org.apache.hive.hcatalog.data.DefaultHCatRecord</span></span><br><span class="line">        <span class="comment">// 继承链：org.apache.hive.hcatalog.data.DefaultHCatRecord-&gt;org.apache.hive.hcatalog.data.HCatRecord-&gt;java.lang.Object</span></span><br><span class="line">        <span class="comment">// Context中还可获取更多参数，如Configuration等。</span></span><br><span class="line">        SqoopRecord record = helper.convertToSqoopRecord(value);</span><br><span class="line">        LOG.info(<span class="string">"==="</span> + record.getFieldMap().toString() + <span class="string">"==="</span>);</span><br><span class="line">        context.write(record, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>主要就是在map方法中将数据打印出来。</p><p>再在前面的main方法中，调用导出数据的命令之前，设置修改后的Mapper：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Logger.getRootLogger().setLevel(Level.INFO);</span><br><span class="line">    BasicConfigurator.configure();</span><br><span class="line">    System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"hdfs"</span>);</span><br><span class="line">    <span class="comment">// 指定Mapper为修改后的类</span></span><br><span class="line">    SqoopHCatUtilities.setExportMapperClass(ModifiedSqoopHCatExportMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    exportData();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>日志中打印的数据如下：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">14587 [LocalJobRunner Map Task Executor #0] INFO blog.ModifiedSqoopHCatExportMapper  - &#x3D;&#x3D;&#x3D;&#123;s&#x3D;s3, id&#x3D;3, ft&#x3D;3.3&#125;&#x3D;&#x3D;&#x3D;</span><br><span class="line">14587 [LocalJobRunner Map Task Executor #0] INFO blog.ModifiedSqoopHCatExportMapper  - &#x3D;&#x3D;&#x3D;&#123;s&#x3D;s4, id&#x3D;4, ft&#x3D;4.4&#125;&#x3D;&#x3D;&#x3D;</span><br><span class="line">...</span><br></pre></td></tr></table></figure></div><h2><span id="参考链接">参考链接</span></h2><ul><li><a href="https://github.com/cloudera/sqoop" target="_blank" rel="noopener">Cloudera Sqoop仓库</a></li><li><a href="http://sqoop.apache.org/docs/1.4.7/SqoopDevGuide.html" target="_blank" rel="noopener">Sqoop Developer’s Guide v1.4.7</a></li></ul>]]></content>
    
    <summary type="html">
    
      处理和过滤Sqoop导出数据的一种方法
    
    </summary>
    
    
    
      <category term="大数据" scheme="http://longwang.live/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Sqoop" scheme="http://longwang.live/tags/Sqoop/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://longwang.live/2020/01/14/hello-world/"/>
    <id>http://longwang.live/2020/01/14/hello-world/</id>
    <published>2020-01-14T13:07:18.176Z</published>
    <updated>2020-01-14T13:07:18.177Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2><span id="quick-start">Quick Start</span></h2><h3><span id="create-a-new-post">Create a new post</span></h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">bash</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure></div><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3><span id="run-server">Run server</span></h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">bash</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure></div><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3><span id="generate-static-files">Generate static files</span></h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">bash</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure></div><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3><span id="deploy-to-remote-sites">Deploy to remote sites</span></h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">bash</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure></div><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
    
  </entry>
  
</feed>
